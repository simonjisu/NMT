{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation Tutorial\n",
    "---\n",
    "\n",
    "Paper Implementation: [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) - Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio (v7 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt \n",
    "import matplotlib.ticker as ticker\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 번역기의 목적\n",
    "\n",
    "소스(source)문장을 타겟(target)문장으로 변환하는 것\n",
    "\n",
    "Examples:\n",
    "\n",
    "|source|target|\n",
    "|--|--|\n",
    "| Nice to meet you| 만나서 반갑습니다 |\n",
    "| I am very happy to meet you | 만나서 참 반가워요| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda d: [t for s in d for t in s]\n",
    "\n",
    "def build_vocab(data, start_tkn=False):\n",
    "    \"\"\"build vocabulary\"\"\"\n",
    "    if start_tkn:\n",
    "        vocab = {'<unk>': 0, '<pad>': 1, '<s>': 2, '</s>': 3}\n",
    "    else:\n",
    "        vocab = {'<unk>': 0, '<pad>': 1}\n",
    "    \n",
    "    words = set(flatten(data))\n",
    "    for t in words:\n",
    "        if vocab.get(t) is None:\n",
    "            vocab[t] = len(vocab) \n",
    "    return vocab\n",
    "\n",
    "def add_pad(data, start_tkn=False):\n",
    "    \"\"\"add padding of sentences in batch to match lenghts\"\"\"\n",
    "    if start_tkn:\n",
    "        data = [['<s>'] + sent + ['</s>'] for sent in data]\n",
    "    max_len = max([len(sent) for sent in data])\n",
    "    data = [sent + ['<pad>']*(max_len-len(sent)) if len(sent) < max_len else sent \\\n",
    "            for sent in data ]\n",
    "    return data\n",
    "\n",
    "def numericalize(data, vocab):\n",
    "    \"\"\"numericalize and turn them into tensor\"\"\"\n",
    "    f = lambda x: [vocab.get(t) if vocab.get(t) is not None else vocab.get('<unk>') for t in x]\n",
    "    data = list(map(f, data))\n",
    "    return data\n",
    "\n",
    "def preprocess(data, vocab, start_tkn=False):\n",
    "    data = add_pad(data, start_tkn=start_tkn)\n",
    "    data = numericalize(data, vocab)\n",
    "    # 텐서플로우는 아래 부분을 수정해야할겁니다. torch.LongTensor 를 제거하고 data 로만 두시고\n",
    "    # 숫자로 치환된 data 를 numpy array 로 출력하신 다음에 진행하면 될것 같습니다. \n",
    "    return torch.LongTensor(data)  \n",
    "\n",
    "def build_batch(src, trg, src_vocab, trg_vocab, is_sort=False):\n",
    "    if is_sort:\n",
    "        sorted_data = sorted(list(zip(src, trg)), key=lambda x: len(x[0]), reverse=True)\n",
    "        src, trg = list(zip(*sorted_data))\n",
    "    src = preprocess(src, src_vocab, start_tkn=False)\n",
    "    trg = preprocess(trg, trg_vocab, start_tkn=True)\n",
    "    if is_sort:\n",
    "        return (src, src.ne(src_vocab.get('<pad>')).sum(1)), trg\n",
    "    return src, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"\"\"Nice to meet you > 만나서 반가워요 \\n I am very happy to meet you > 만나서 참 반가워요\"\"\".splitlines()\n",
    "dataset = [s.strip().split('>') for s in dataset]\n",
    "src, trg = [[sent.split() for sent in x] for x in zip(*dataset)]\n",
    "src_vocab = build_vocab(src)\n",
    "trg_vocab = build_vocab(trg, start_tkn=True)\n",
    "(inputs, lengths), targets = build_batch(src, trg, src_vocab, trg_vocab, is_sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 8, 3, 9, 2, 7, 5],\n",
       "        [6, 2, 7, 5, 1, 1, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 6, 4, 5, 3],\n",
       "        [2, 6, 5, 3, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters for Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED = 20  # embedding_size\n",
    "HIDDEN = 60  # hidden_size\n",
    "ENC_N_LAYER = 3  # encoder number of layers\n",
    "L_NORM = True  # whether to use layernorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, n_layers, layernorm=False, bidirec=False):\n",
    "        super(Encoder, self).__init__()        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_direction = 2 if bidirec else 1\n",
    "        self.layernorm = layernorm\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, bidirectional=bidirec, \n",
    "                          batch_first=True)\n",
    "        if layernorm:\n",
    "            self.l_norm = nn.LayerNorm(embed_size)\n",
    "        \n",
    "    def forward(self, inputs, lengths):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - inputs: B, T_e\n",
    "        - lengths: B, (list)\n",
    "        Outputs:\n",
    "        - outputs: B, T_e, n_directions*H\n",
    "        - hiddens: 1, B, n_directions*H\n",
    "        \"\"\"\n",
    "        assert isinstance(lengths, list), \"lengths must be a list type\"\n",
    "        # B: batch_size, T_e: enc_length, M: embed_size, H: hidden_size\n",
    "        inputs = self.embedding(inputs) # (B, T_e) > (B, T_e, m)\n",
    "        if self.layernorm:\n",
    "            inputs = self.l_norm(inputs)\n",
    "        \n",
    "        packed_inputs = pack_padded_sequence(inputs, lengths, batch_first=True)\n",
    "        # packed_inputs: (B*T_e, M) + batches: (T_e)\n",
    "        packed_outputs, hiddens = self.gru(packed_inputs)\n",
    "        # packed_outputs: (B*T_e, n_directions*H) + batches: (T_e)\n",
    "        # hiddens: (n_layers*n_directions, B, H)\n",
    "        outputs, outputs_lengths = pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        # output: (B, T_e, H) + lengths (B)\n",
    "        hiddens = torch.cat([h for h in hiddens[-self.n_direction:]], 1).unsqueeze(0)\n",
    "        # hiddens: (1, B, n_directions*H)\n",
    "        return outputs, hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(src_vocab), EMBED, HIDDEN, ENC_N_LAYER, L_NORM, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 7, 120]), torch.Size([1, 2, 120]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output, enc_hidden = encoder(inputs, lengths.tolist())\n",
    "enc_output.size(), enc_hidden.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_embedding = nn.Embedding(len(TRG.vocab), config.EMBED).to(DEVICE)\n",
    "sos = torch.LongTensor([2]*32).unsqueeze(1).to(DEVICE)\n",
    "dec_input = dec_embedding(sos)\n",
    "dec_input.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention\"\"\"\n",
    "    def __init__(self, hidden_size, method='general', device='cpu'):\n",
    "        super(Attention, self).__init__()\n",
    "        \"\"\"\n",
    "        * hidden_size: decoder hidden_size(H_d=encoder_gru_direction*H)\n",
    "        methods:\n",
    "        - 'dot': dot product between hidden and encoder_outputs\n",
    "        - 'general': encoder_outputs through a linear layer \n",
    "        - 'concat': concat (hidden, encoder_outputs) ***NOT YET***\n",
    "        - 'paper': concat + tanh ***NOT YET***\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.device = device\n",
    "        self.hidden_size = hidden_size \n",
    "        if self.method == 'general':\n",
    "            self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hiddens, enc_outputs, enc_lengths=None, return_weight=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - hiddens(previous_hiddens): B, 1, H_d\n",
    "        - enc_outputs(enc_outputs): B, T_e, H_d\n",
    "        - enc_lengths: real lengths of encoder outputs\n",
    "        - return_weight = return weights(alphas)\n",
    "        Outputs:\n",
    "        - contexts: B, 1, H_d\n",
    "        - attns: B, 1, T_e\n",
    "        \"\"\"\n",
    "        hid, out = hiddens, enc_outputs\n",
    "        # Batch(B), Seq_length(T)\n",
    "        B, T_d, H = hid.size()\n",
    "        B, T_e, H = out.size()\n",
    "        \n",
    "        score = self.get_score(hid, out)\n",
    "        # score: B, 1, T_e\n",
    "        if enc_lengths is not None:\n",
    "            mask = self.get_mask(B, T_d, T_e, enc_lengths)  # masks: B, 1, T_e\n",
    "            score = score.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attns = torch.softmax(score, dim=2)  # attns: B, 1, T_e\n",
    "        contexts = attns.bmm(out)\n",
    "        if return_weight:\n",
    "            return contexts, attns\n",
    "        return contexts\n",
    "            \n",
    "    def get_score(self, hid, out):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - hid(previous_hiddens): B, 1, H_d \n",
    "        - out(enc_outputs): B, T_e, H_d\n",
    "        Outputs:\n",
    "        - \n",
    "        \"\"\"\n",
    "        if self.method == 'dot':\n",
    "            # bmm: (B, 1, H_d) * (B, H, T_e) = (B, 1, T_e)\n",
    "            score = hid.bmm(out.transpose(1, 2))\n",
    "            return score\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            # linear: (B, T_e, H_d) > (B, T_e, H_d)\n",
    "            # bmm: (B, 1, H_d) * (B, H_d, T_e) = (B, 1, T_e)\n",
    "            score = self.linear(out)\n",
    "            score = hid.bmm(score.transpose(1, 2))\n",
    "            return score\n",
    "\n",
    "    def get_mask(self, B, T_d, T_e, lengths):\n",
    "        assert isinstance(lengths, list), \"lengths must be list type\"\n",
    "        mask = torch.zeros(B, T_d, T_e, dtype=torch.uint8).to(self.device)\n",
    "        for i, x in enumerate(lengths):\n",
    "            if x < T_e:\n",
    "                mask[i, :, x:].fill_(1)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 1024]), torch.Size([32, 1, 30]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_direc = 2\n",
    "attention = Attention(enc_direc*config.HIDDEN, method='general', device=DEVICE).to(DEVICE)\n",
    "contexts, attns = attention(enc_hidden.transpose(0,1), \n",
    "                            enc_output, lengths.tolist(), return_weight=True)\n",
    "contexts.size(), attns.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, n_layers=1, sos_idx=2, drop_rate=0.0, layernorm=False, method='general', teacher_force=False, device='cpu', return_w=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        self.sos_idx = sos_idx\n",
    "        self.return_w = return_w\n",
    "        self.teacher_force = teacher_force\n",
    "        self.layernorm = layernorm\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.attention = Attention(hidden_size, method=method, device=device)\n",
    "        self.gru = nn.GRU(embed_size+hidden_size, hidden_size, n_layers, bidirectional=False, \n",
    "                          batch_first=True)\n",
    "        self.linear = nn.Linear(2*hidden_size, vocab_size)\n",
    "        if layernorm:\n",
    "            self.l_norm = nn.LayerNorm(embed_size)\n",
    "    \n",
    "    def start_token(self, batch_size):\n",
    "        sos = torch.LongTensor([self.sos_idx]*batch_size).unsqueeze(1).to(self.device)\n",
    "        return sos\n",
    "    \n",
    "    def init_hiddens(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.n_layers, self.hidden_size).to(self.device)\n",
    "    \n",
    "    def forward(self, hiddens, enc_output, enc_lengths=None, max_len=None, targets=None, \n",
    "                is_eval=False, is_test=False, stop_idx=3):\n",
    "        \"\"\"\n",
    "        * H_d: decoder hidden_size = encoder_gru_directions * H\n",
    "        * M_d: decoder embedding_size\n",
    "        Inputs:\n",
    "        - hiddens: last encoder hidden at time 0 = 1, B, H_d \n",
    "        - enc_output: encoder output = B, T_e, H_d\n",
    "        - enc_lengths: encoder lengths = B\n",
    "        - max_len: max lenghts of target = T_d\n",
    "        Outputs:\n",
    "        - scores: results of all predictions = B*T_d, vocab_size\n",
    "        - attn_weights: attention weight for all batches = B, T_d, T_e\n",
    "        \"\"\"\n",
    "        if is_test:\n",
    "            is_eval=True\n",
    "        inputs = self.start_token(hiddens.size(1))  # (B, 1)\n",
    "        inputs = self.embedding(inputs) # (B, 1, M_d)\n",
    "        if self.layernorm:\n",
    "            inputs = self.l_norm(inputs)\n",
    "        inputs = self.dropout(inputs)\n",
    "        # match layer size: (1, B, H_d) > (n_layers, B, H_d)\n",
    "        if hiddens.size(0) != self.n_layers:\n",
    "            hiddens = hiddens.repeat(self.n_layers, 1, 1)\n",
    "        # prepare for whole target sentence scores\n",
    "        scores = []  \n",
    "        attn_weights = []\n",
    "        for i in range(1, max_len):\n",
    "            # contexts[c{i}] = alpha(hiddens[s{i-1}], encoder_output[h])\n",
    "            # select last hidden: (1, B, H_d) > transpose: (B, 1, H_d) > attention\n",
    "            contexts = self.attention(hiddens.transpose(0, 1), enc_output, enc_lengths, \n",
    "                                      return_weight=self.return_w)\n",
    "    \n",
    "            if self.return_w:\n",
    "                attns = contexts[1]  # attns: (B, seq_len=1, T_e) \n",
    "                contexts = contexts[0]  # contexts: (B, seq_len=1, H_d)\n",
    "                attn_weights.append(attns)\n",
    "                \n",
    "            # gru_inputs = concat(embeded_token[y{i-1}], contexts[c{i}]): (B, seq_len=1, H_d+M_d)\n",
    "            gru_inputs = torch.cat((inputs, contexts), 2)\n",
    "            \n",
    "            # gru: s{i} = f(gru_inputs, s{i-1})\n",
    "            # (B, 1, M_d+H_d) > (n_layers, B, H_d)\n",
    "            _, hiddens = self.gru(gru_inputs, hiddens)            \n",
    "            \n",
    "            # scores = g(s{i}, c{i})\n",
    "            # select last hidden: (1, B, H_d) > transpose: (B, 1, H_d) > concat: (B, 1, H_d + H_d) >\n",
    "            # output linear : (B, seq_len=1, vocab_size)\n",
    "            score = self.linear(torch.cat((hiddens.transpose(0, 1), contexts), 2))\n",
    "            scores.append(score)\n",
    "    \n",
    "            if (self.teacher_force and not is_eval):\n",
    "                selected_targets = targets[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                selected_targets = None\n",
    "            \n",
    "            inputs, stop_decode = self.decode(is_tf=self.teacher_force, \n",
    "                                              is_eval=is_eval,\n",
    "                                              is_test=is_test,\n",
    "                                              score=score, \n",
    "                                              targets=selected_targets, \n",
    "                                              stop_idx=stop_idx)\n",
    "            if stop_decode:\n",
    "                break\n",
    "            \n",
    "        scores = torch.cat(scores, 1).view(-1, self.vocab_size)  # (B, T_d, vocab_size) > (B*T_d, vocab_size)\n",
    "        if self.return_w:\n",
    "            return scores, torch.cat(attn_weights, 1)  # (B, T_d, T_e)\n",
    "        return scores\n",
    "    \n",
    "    def decode(self, is_tf, is_eval, is_test, score, targets, stop_idx):\n",
    "        \"\"\"\n",
    "        - for validation: if is_tf, set 'is_eval' True, else False\n",
    "        - for test evaluation: set 'is_tf' False and set 'is_eval' True\n",
    "        \"\"\"\n",
    "        stop_decode = False\n",
    "        if is_test:\n",
    "            # test\n",
    "            preds = score.max(2)[1]\n",
    "            if preds.view(-1).item() == stop_idx:\n",
    "                stop_decode = True\n",
    "            inputs = self.embedding(preds)\n",
    "        else:\n",
    "            # train & valid\n",
    "            if is_tf and not is_eval:\n",
    "                assert targets is not None, \"target must not be None in teacher force mode\"\n",
    "                inputs = self.embedding(targets)\n",
    "            else:\n",
    "                preds = score.max(2)[1]\n",
    "                inputs = self.embedding(preds)\n",
    "\n",
    "        if self.layernorm:\n",
    "            inputs = self.l_norm(inputs)\n",
    "        inputs = self.dropout(inputs)\n",
    "        return inputs, stop_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(len(TRG.vocab), config.EMBED, encoder.n_direction*config.HIDDEN, n_layers=config.DEC_N_LAYER,\n",
    "                  drop_rate=config.DROP_RATE, method=config.METHOD, layernorm=config.L_NORM, \n",
    "                  sos_idx=TRG.vocab.stoi['<s>'], teacher_force=config.TF, \n",
    "                  return_w=config.RETURN_W, device=DEVICE).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([992, 41652])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o= decoder(hiddens=enc_hidden, enc_output=enc_output, max_len=targets.size(1),\n",
    "               targets=targets)\n",
    "o.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function = nn.NLLLoss(ignore_index=TRG.vocab.stoi['<pad>'])\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=TRG.vocab.stoi['<pad>'])\n",
    "\n",
    "enc_optimizer = optim.Adam(encoder.parameters(), \n",
    "                               lr=config.LR, \n",
    "                               weight_decay=config.LAMBDA)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(), \n",
    "                           lr=config.LR * config.DECLR, \n",
    "                           weight_decay=config.LAMBDA)\n",
    "enc_scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.1,\n",
    "                                               milestones=[int(config.STEP / 4), \n",
    "                                                           int(config.STEP / 2),\n",
    "                                                           int(3 * config.STEP / 4)],\n",
    "                                               optimizer=enc_optimizer)\n",
    "dec_scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.1,\n",
    "                                               milestones=[int(config.STEP / 4), \n",
    "                                                           int(config.STEP / 2),\n",
    "                                                           int(3 * config.STEP / 4)],\n",
    "                                               optimizer=dec_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.6426, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_function(o, targets[:, 1:].contiguous().view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_step(enc, dec, loader, loss_function, enc_optimizer, dec_optimizer):\n",
    "    enc.train()\n",
    "    dec.train()\n",
    "    losses = []\n",
    "    for i, batch in enumerate(loader):\n",
    "        inputs, lengths = batch.src\n",
    "        targets = batch.trg\n",
    "\n",
    "        enc.zero_grad()\n",
    "        dec.zero_grad()\n",
    "        \n",
    "        enc_output, enc_hidden = enc(inputs, lengths.tolist())\n",
    "        outputs = dec(enc_hidden, enc_output, lengths.tolist(), \n",
    "                      targets.size(1), targets, is_eval=False)\n",
    "        loss = loss_function(outputs, targets[:, 1:].contiguous().view(-1))\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(enc.parameters(), 50.0)  # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(dec.parameters(), 50.0)  # gradient clipping\n",
    "        \n",
    "        enc_optimizer.step()\n",
    "        dec_optimizer.step()\n",
    "        print(' > [{}/{}] train_loss {:.4f}'.format(i, len(loader), loss.item()))\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > [0/11839] train_loss 10.3431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.34306812286377"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_step(encoder, decoder, train_loader, loss_function, enc_optimizer, dec_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd() + '/model')\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt \n",
    "import matplotlib.ticker as ticker\n",
    "import settings\n",
    "from train import import_data, build_model, validation, build_config_file\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "# DEVICE = 'cuda' if config.USE_CUDA else 'cpu'\n",
    "DEVICE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Language: 40781 words, Target Language: 26320 words\n",
      "Training Examples: 160477, Validation Examples: 776\n",
      "Building Model ...\n"
     ]
    }
   ],
   "source": [
    "SRC, TRG, train, _, test, _, _, test_loader = import_data(config=settings, device=DEVICE, is_test=True)\n",
    "enc, dec, loss_function, *_ = build_model(config=settings, src_field=SRC, trg_field=TRG, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.load_state_dict(torch.load(settings.SAVE_ENC_PATH))\n",
    "dec.load_state_dict(torch.load(settings.SAVE_DEC_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss is 5.9735\n"
     ]
    }
   ],
   "source": [
    "# set TF to False when testing\n",
    "print(\"test loss is {:.4f}\".format(validation(settings, enc, dec, test_loader, loss_function)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_data, max_len, src_field, trg_field, stop_token='</s>'):\n",
    "    f = lambda x: trg_field.vocab.itos[x]\n",
    "    test_ex = np.random.choice(test_data.examples)\n",
    "    src_sent = test_ex.src\n",
    "    trg_sent = test_ex.trg\n",
    "    inputs, lengths = src_field.numericalize(([src_sent], [len(src_sent)]))\n",
    "    enc_output, enc_hidden = enc(inputs, lengths.tolist())\n",
    "    outputs, attns = dec.forward(enc_hidden, enc_output, lengths.tolist(), max_len, \n",
    "                                 targets=None, is_test=True, \n",
    "                                 stop_idx=trg_field.vocab.stoi[stop_token])\n",
    "    preds = outputs.max(1)[1]\n",
    "    translated = list(map(f, preds.tolist()))\n",
    "    \n",
    "    print(\" < input: {}\".format(' '.join(src_sent)))\n",
    "    print(\" > target: {}\".format(' '.join(trg_sent)))\n",
    "    print(\" > predict: {}\".format(' '.join(translated[:-1])))\n",
    "    return attns.squeeze(0).detach(), (src_sent, trg_sent, translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence + ['</s>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " < input: und drittens benötigt man begeisterung .\n",
      " > target: and third , you need desire .\n",
      " > predict: and third need you need to go .\n"
     ]
    }
   ],
   "source": [
    "attns, pairs = evaluate(train, settings.MAX_LEN, src_field=SRC, trg_field=TRG, stop_token='</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_attention(input_sentence=pairs[0], output_words=pairs[2], attentions=attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_showattns(test_data, max_len, src_field, trg_field, stop_token='</s>'):\n",
    "    attns, pairs = evaluate(test_data, max_len, src_field=src_field, trg_field=trg_field, stop_token=stop_token)\n",
    "    show_attention(input_sentence=pairs[0], output_words=pairs[2], attentions=attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " < input: sie hatten nie wirklich darüber nachgedacht , ein unternehmen zu gründen .\n",
      " > target: they had never really thought about starting up a business .\n",
      " > predict: they never really thought about a good job .\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAE4CAYAAABL+QhEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xm8HFWd/vHPQwDDjhLcWAQRRUQMEFDcwHVwZVQUQR1BJfpzG5fRYWZcGGac0dEZx23EqCgyLoC4REVBWR0UIQEEgqAIKuAaZHMBJPf5/VHVpHO5SyVddau6+3nnVa90VZ97+ptO8u3Tp84i20RExPBbr+0AIiKiHknoEREjIgk9ImJEJKFHRIyIJPSIiBGRhB4RMSKS0CMiRkQSekTEiEhCj4gYEUnoEUNC0nMl/UTSzZJukXSrpFvajiu6Q5n6HzEcJF0FPMv2j9qOJbopLfQYCpJOr3JtxP0myTxmsn7bAUTMRNJ8YGNggaR7Aiqf2hzYprXA2rFM0gnAV4Dbexdtf6m9kKJLktCj614JvAG4P7Cc1Qn9FuDDbQXVks2BPwFP7btmIAk9gPShx5CQ9DrbH2o7joguS0KPoSHp0cAO9H2ztP2Z1gKaY5IeDHwUuI/t3STtDjzb9r+2HFp0RBJ6DAVJxwM7ARcDq8rLtv369qKaW5LOBt4CfMz2HuW1y2zv1m5k0RXpQ49hsQjY1ePdAtnY9vmS+q/d2VYw0T0ZthjD4jLgvm0H0bKVknaiuBGKpIOAX7UbUnRJulyi0yR9jSKBbQYsBM5nzSF7z24ptDkn6YHAEuDRwI3ANcCLbf+szbiiO5LQo9Mk7TfT87bPnqtYukLSJsB6tm9tO5boliT0GAqSdgR+Zfu28nwjitEeP2s1sDkkaUvgb7j7SJ+xuTEcM8tN0RgWJ1F0NfSsKq/t3U44rTgFOA+4FJhoOZbooCT0GBbr276jd2L7DkkbthlQC+bbflPbQUR3ZZRLDIvfSbrrBqikA4GVLcbThuMlHSHpfpLu1TvaDiq6I33oMRTK4XqfpVjTRcC1wN/YvqrVwOaQpNcA7wJuohy6SDG56oHtRRVdkoQeQ0XSpgC2/9B2LHNN0tXAPrbH7ZtJVJQ+9Bgakp4BPAyY35stafvoVoOaW1dRrLYYMaUk9BgKko6hWBf9CcAngIMoJhmNkz8CF0s6kzUnV2XYYgDpcokhIekS27v3/b4p8E3bj2s7trki6aVTXbd93FzHEt2UFnoMiz+Xv/9J0v2BG4D7tRjPnEvijtkkocew+Ho5U/K9wIUUozw+0W5Ic0vSY4CjgAdQ/N8VGeUSfdLlEkNH0j0oJtnc3HYsc0nSFcAbKbbi660Jj+0bWgsqOiUt9Og0Sc+d4blx2yD5ZtvfbDuI6K600KPTJH2qfHhvirVczijPnwB8z/YzWwlsDknas3z4AmAexabQ/aNcLmwjrtlI2obV3UMA2D6nvYhGXxJ6DAVJpwEvtf2r8vx+wKdt/1W7kTWvHKY4Hdt+4pwFU5Gk9wAHA5ez5paBY7N+fRuS0KM2kuYBr7f9/gbq/pHth/adrwes6L8W3SHpSmB327fPWjhqk8W5oja2VwGHNFT96ZJOlXSYpMOAbwDfaei1OknSfSR9UtI3y/NdJb287bimcTWwQdtBjJu00MeUpK2BI7j7ZgkvG7De91P8Rz6BYmZjr96B+3klPQd4fHl6ju0vD1rnMCkT+aeAf7L9CEnrAxfZfnjLod2NpJOBRwCnk1mtcyajXMbXV4HvUrRyV81Sdm0sLH/vX2PFQB39vBcCt9r+jqSNJW02ZtuwLbB9oqR/ALB9p6Q6/+7qtLQ8Yg4loY+vjW3/fd2V2n5C3XUCSDoCWAzcC9gJ2AY4BnhSE6/XUX+UtBXl0rmSHgV0ciy+7ePKbQK3t31l2/GMi/Shj6+vS3p63ZU22M/7GuAxwC0Atn9CMZRxnLyJotW7k6Rzgc8Ar2s3pKlJehZwMfCt8nyhpLTYG5Y+9DEj6VaKFp6ATSj6N//C6mnkmw9YfyP9vJJ+YPuRki6yvUdZ74W2dx+k3mFT/rkfQvH3daXtv7Qc0pQkLafoZjvL9h7ltcts79ZuZKMtXS5jxvZmDb9EU/28Z0v6R2AjSU8BXg18rYZ6h80+rL6RvWc5W/Yz7YY0pb/Yvrm3bn0pG1s3LF0uNZL0GEnflvRjSVdLuqbcZaZzJD1H0hZ951tK+usaqm6qn/dI4HcUO96/EjgFeFsN9Q4NSccD7wMeC+xdHotaDWp6KyQdCsyTtLOkDwHfazuoUZculxoN0+JJki62vXDStYt6X48HqHdP4EPAbsBlwNbAQbYvGaTeKCZXAbt6CP7TStoY+CfgqRTdQ6cC/2L7tlYDG3FJ6DXq9fO2HUcVvY0iJl27tI4xzU3080q6lNUbI/fcDCwD/rWLH5p1k3QSxUzcX7UdS3RT+tDrdaak9zIciyctk/RfwEfK89dQfLMYiKT5FP3bj6VIwN+VdEwNLbNvUnzr+Vx5/kKKLel+DXwaeNaA9Q+DBcDlks5nzX9fnVkfRdLXuPsH7126FOsoSgu9RtMsotTVxZM2Ad4OPLm89G2Klu4fp/+pSvWeCNwK/G956VBgS9vPH7DeC23vOdW1ur5ZdJ2k/aa6bvvsuY5lOn0xPhe4L6v/HRwC/Mb2G1sJbEwkoUetJF1ue9fZrq1DvT8EjrB9fnm+N/CJcmjkwH3/XVcufPadpiZu1U3SMtuLZrsW9UqXS40k3Qf4N+D+tp8maVdgX9ufbDm0u0j6b9tvmO6rcQ1fiS+U9Cjb55Wv90iKfu5BvQI4ttwcWhQTjF5RftP49xrq7zTbqyRNSNpiSHZq2kTSA21fDSBpR4p5D9GgtNBrNAyLJ0nay/byur++99203IDihugvyvMHAFcM2kLve50tyjiHIanVStJXgT0ousf6Fz7r3IJXkg4AllCsuiiKfwevtH1qq4GNuCT0Gkm6wPbe/V0AUw0PHEWSHjDT87Z/PmD99wCex91Xhzx6up8ZNZJeOtV128fNdSxVlH9nu5SnV2Rt9Oaly6VenV88aZrhf3dZ16n0vYQtaft1DG02X6V4L5fTN8JjnAzhgld7sfoD+BEdntU6MtJCr9E0k2qeb/uHrQbWp6mWtKRdbF/R94EhYD6wI8VY9IetS7199Y/9OiDlglfvAza0vaOkhcDRXRwKWM5q3Yliga7+Leg61z00StJCr9cKYD/6JtXQseUV+lrST/OkHeQlvYpiSdp18WaKUShr3C8oP+RevY519vuepIfbvrSGuobVURRruZwFYPtiSQ9sM6AZLGJIZrWOkk4lmxHwfdt32l5h+7JyhuT32w5qGm+XdNf4eElvBQ4coL4pPwjKSVXrPHtW0qWSLqGYqHShpCslXdJ3fZz8ZYqbwV1d8OoyinHoMYfSQq+BpPtSbLiwkaQ9KFrnAJtTzGbsomdTrIn+FuAAiptXgyT0lwDLJb2Z1X306wF7Ar8coN5nDvCzo2aNBa+A19PdBa86P6t1FKUPvQbl6IPDKL5mXsDqhH4LcJztL7UU2owk3ZtiC7rlwMvq+Hos6Z19p3cCPwNOHnTqf3mDeUVvyzlJmwMPtf2DQeodJpMWvILVC1517ibxMMxqHUVJ6DWS9Fbb/zHp2o62r2krpsm0eoOLng0pEq8ZcIOLcjbje2z/3WBRTln3RcCevQ8dSesByyYvBzDKJD3f9kmzXYvxlT70er1wimtfnPMoZlBucLEFsJvtzW3Pt72p7c0GSeZl3asotolrgvq/QdieYPy6DP+h4rXWlN+ckHSrpFvK4zZJqyTd0nZ8o27c/kM0QtIuwMOALSQ9t++pzSmG7g1af61LCti2pG8ATcxgvVjF3pEnseZsxkG7na6W9Hrgo+X5qylmIXZO3ZOgJD0NeDqwjaQP9j21OcW3qy75PPAM9+2MJUkU92ce1VpUY2IsW+iSNpb0dkkfL893ljTIzbeHUNy825JiGdfesSdwxKDxUiwPeypw//L8x8AbBqzzwnKBq7rNB26g2E+y9z7UcWPzVcCjgeuB6yhGziyuod4mfJUigd1J8aHWO9bVLynWw7mN4n5H71gK/NVAkdbvFZMvuPAVuhfryBnLPnRJJ1D8h/gb27uVN5u+N+gUfUn72q59mGITSwqo2F3pQcDPKZJNb5Posdp0uQlNTYKStIE7uil0j6QTbb9g0jfV9SgGDOxne9+WQhsL49rlspPtgyUdAmD7T+XXwkFdJOk1FN0vd3W12H7ZgPU2saRAI60lFRtcvJya34Om6m1IU5Og9pF0FMVCV+uz+kO4M5OLbL+gfNi/4UhvtNMgw2KjgnFN6HeUa2L0EuRO1LM+yPHAFRTJ8mjgRcCPaqj3TRRfr3eSdC7lPp3rUpGkzW3fQrEJRROaeg+aqrcJjwUOk3QNxb+rur79fJIp9qwdhKRPMfUyyuv8QVmOdrrE9vsHiS3W3rh2uTyFYsf4XYHTKEZmHGb7rAHrvcj2Hir365S0AfBd2wPfDFJN+3RK+rrtZ5bJprfmSs/Arb2m3oMm39u6TbdeTg0rTta+Z62k5/WdzgeeA/xy0DVXJJ1ve5+Bgou1NpYtdNvflnQhxV13AX9re2UNVfeS7E2SdqPY7/Le61qZpCfaPmNSfyTAg8uV69Z65Ijt3g3Kc4GzKZLiFesa4xRqfQ/moN7azMG3n9r3rLV9cv+5pM8D/7fOEa52rqQPAyew5minLu6vOzLGKqH3rQjYm4zS2z19e0nb1/CPbYmke1K0/pcCm1Ls27mu9gPOYHV/ZO/rlMrHgwwF/CTwOOBDZZfThRTJ/QMD1An1vwdN11unz1GM6FnOmn9XlOeD9nX3Wud79dVtihFFddmZej4oezfs/7n8vYlYY5Kx6nKRtMT2Yq25mXP/ZJWB/rFNGn+8wepqB9uEobwhOHlccx31zgP2Bp5AMSzwz7Z3mfmnpq3rTVNdLn+37f9atyjvqr+R97YJ5SzWFwE72j5axRrx9xt0mYJJyyr0DPQe9M0c7iXcXwNHDjpvoG9Nn/4PtFsoZvdePEjdMb2xaqHb7o1b/ijwLdu3SHo7xXjxf6nhJZrahOErwE0UrejemigDfRJLOp1ij8fvA98F9rb92wGq7E0keQjFh8TS8vxZwPkD1NszTBtcfIRiFcQnUtzAvRU4meJ9GcQf+h7Pp/g2MOiN4QuA/7T9jd4FSUsY7NsfFN8iFlH8OxBFrJcAr5R00uQlMqImtsfuoLgDD8VohDOBZwA/qKHeyxqKt/Z6gfcD51DsT3kURfLZqIZ6zwE26zvfDDini+9BUwdwYfn7RX3XftjA69wDOGvAOq6muJfyjsnx1/DvYNO+803L19kIuLztv6NRPcZypiirh3w9A/i4i9bJhjXU+z1JTUynr71e22+0/XjguRQzOz9F8S1gUPcB7ug7v6O8Nqim3ts1SLpf2b0ziL+U3Vm9YbFb08y65RsD2w5Yx03Ak4D7Svqayk24a3Bv1vwm9RfgPrb/TPe/YQ2tsepy6XO9pI8BTwHeU/4HXucPN63edm194HBJV1Pv+OPaxzVLei3FTdG9KCZ9HEvR9TKozwDnS/pyef7XFEsXDKqpsd2THU8x3v9kr/uqkR8EvgzcW9K7KOYMvG3QwLTmfrDzKOYjDHoPQbbvBF4t6TCKES73HLBOgM8CP5D01fL8WcDnJG0CXF5D/WuQdF/bv6673mEzVjdFe8qp/gcAl9r+iaT7AQ+3fdo61tf0jve1j2uW9HcUCXx5+R+6NuUooseVp+fYvqiGOhsZ2z3Na4li+7QVA9SxC0XLV8DptgeeBDXpPbgT+M2gf3eSXmn7Y33newGvcQ0zcCUtYvXqm+faXjZonTO81jdsP6Op+ofFWCb0iIhRNK596BERI2fsE7qkRpZgTb3DFeuw1TtMsQ5jvU2TdKyk30q6bJrnJemDkq5SsSl6pZ25xj6h09ya2ql3uGIdtnqHKdZhrLdpn6a4jzedp1HM2t2Z4s/40RnK3iUJPSJijtk+B/j9DEUOBD7jwnnAluXgjRmN5LDFBQsWeIcddqhUdvvtt2fRokWV7gwvX758reKQ1Mgd52Gqd5hiHbZ6hynWDtS70vbWg7zOAQcc4JUrq63ht3z58hWsntUNsMT2krV4uW2Aa/vOryuv/Wrq4oWRTOg77LADy5bVP0JKteyBEREtGHh468qVKyvnFUm32V406GuurZFM6BERTZjDYd7XA9v1nW9bXptR+tAjIiowsGpiotJRg6XA35SjXR4F3Gx7xu4WSAs9IqIi48EWOb1LuZHI/sACSdcB76RcFtr2McApwNOBq4A/AYdXqTcJPSKiCsNETT0utg+Z5XkDr1nbepPQIyIq6vpSKUnoEREVGJjoeEJv/KaopC0lvbp8vL+krzf9mhERTai60URb5mKUy5bAq+fgdSIiGmN7Lke5rJO5SOjvptgw4GLgvcCmkr4o6QpJny3XnkbSXpLOlrRc0qnlzjE7SbqwV5GknfvPIyLmUlrocCTwU9sLgbcAewBvAHYFHgg8RtIGwIeAg2zvRbF7zrts/xS4WdLCsq7DKbZKuxtJiyUtk7Tsd7/7XbN/oogYS674qy1t3BQ93/Z1AGWrfQeKfQ13A75dNtjnsXrNgk9QbOv2JuBgYJ+pKi3XSVgCVF6bJSKiquKmaNtRzKyNhN6/QeyqMgYBK2zvO0X5kykG3Z9BsV3aDc2HGBFxd10ftjgXXS63ApvNUuZKYGtJ+wJI2kDSwwBs3wacSrEe8JTdLRERjRuCm6KNt9Bt3yDp3HJnjj8Dv5mizB2SDgI+KGmLMq7/Bnqb9H4WeA6wTps4R0QMynS/hT4nXS62D53m+mv7Hl8MPH6aKh4LfMr2qgbCi4iopOsTizo/U1TSl4GdgCe2HUtEjLe00Adk+zltxxARUedqi03pfEKPiOgC17jaYlOS0CMiKppocQRLFUnoEREVDMNqiyOZ0JcvX97Ihs5N3BDJxtMRwyM3RSMiRoGdFnpExKhICz0iYgQYWJWEHhExGtJCj4gYEUnoEREjwLkpGhExOtJCj4gYEUnoDZG0vu07244jIsZDMcql21P/a92xSNIOkn4k6eOSVkg6TdJGknaS9C1JyyV9V9IukraQ9HNJ65U/u4mka8vdiu5WvizzaUnHSPoB8B91xh4RMZsJVzva0kQLfWfgENtHSDoReB5wOPAq2z+R9Ejgf2w/sdwkej/gTOCZwKm2/yJpyeTyrF4PfVvg0ZM3u5C0GFjcwJ8nIgLssexyuabcfQhgObAD8GjgpL51S+5R/n4CcDBFQn8h8D+SNp2hPMBJU+1cZHsJsARAUrff9YgYOuO6Bd3tfY9XAfcBbrK9cIqyS4F/k3QvYC/gDGCTGcoD/LHOYCMiqur6sMVa+9CncQtwjaTnA6jwCADbfwAuAD4AfN32KtvTlo+IaJPLbpfZjrbMRUIHeBHwckk/BFYAB/Y9dwLw4vL3KuUjIuacbVZNTFQ62lJrl4vtnwG79Z2/r+/pA6b5mS8CmnTtmqnK2z6sjjgjItZF9hSNiBgR2VM0ImIEDMMol7nqQ4+IGHp13hSVdICkKyVdJenIKZ7fXtKZki6SdImkp89WZ1roERFVlDdF6yBpHvAR4CnAdcAFkpbavryv2NuAE21/VNKuwCkU83qmlRZ6REQFvS6Xmlro+wBX2b7a9h3AF7j7aD4Dm5ePtwB+OVulaaGvhb6Zq7Vpqk+uiVgjxt1aTCxaIGlZ3/mScjZ7zzbAtX3n1wGPnFTHUcBpkl5HMeHyybO9aBJ6RERFazFscaXtRQO+3CHAp23/p6R9geMl7WZPv+RjEnpEREU1fqG+Htiu73zb8lq/l1POx7H9fUnzgQXAb6erNH3oEREVmKLLpcpRwQXAzpJ2lLQhxeKESyeV+QXwJABJDwXmA7+bqdK00CMiqqhxlIvtOyW9FjgVmAcca3uFpKOBZbaXAm8GPi7pjRSfJ4d5lptuSegRERXUPbHI9ikUQxH7r72j7/HlwGPWps4k9IiIijJTdC2UW8wdVD4+S9Kgd4kjImpTYx96IxproasYCK2ZhthERAwPd361xSY2ib5S0meAy4CXSPq+pAslnVRuL4ekd0i6QNJlkpZohlkwkl4m6b/7zo+Q9P46446ImI1d/WhLE10uO1Ns6rwfxTjKJ9veE1gGvKks82Hbe9veDdiIYoPo6ZwIPEvSBuX54cCxDcQdETGjsdrgovRz2+dJeiawK3Bu2QDfEPh+WeYJkt4KbAzci2JXoq9NVZntP0g6A3impB8BG9i+dHI5SYuBxbX/aSIiWD0OvcuaSOi9TZwFfNv2If1PlrOd/gdYZPtaSUdRDJifySeAfwSuAD41VYFynYQl5Wt0+12PiKE0zqNczgMeI+lBAJI2kfRgVifvlWWf+kGzVWT7BxTTZA8FPt9QvBER06u40mKbSb+xUS62fyfpMODzku5RXn6b7R9L+jjFTdNfU0yBreJEYKHtG+uPNiKigo630JveJPoMYO8pyr2NYvH2ydcP63u8/6SnHwtkdEtEtGZiVbcTeqcmFk1F0paSfgz82fbpbccTEeOpGJI4pl0udbF9E/DgtuOIiOj6TdHOJ/SIiG5ot/VdRRJ6RERFnkhCj4gYer0+9C5LQm9ZU5s5Z/PpiPq5xWn9VSShR0RU1PEGehJ6REQldvrQIyJGRfrQIyJGQN17ijYhCT0ioqIk9IiIUWDjVRnlEhExErreQp9xca5yYaxXl4/3l/T1JoMp9yS9bJrnDpN0/yZfPyJiJsO+p+iWwKvnIpAKDgOS0COiFb2bol1ebXG2hP5uYCdJFwPvBTaV9EVJV0j6rMppg5KeJOkiSZdKOra3oYWkn0laUD5eJOms8vHWkr4taYWkT0j6ea8cME/Sx8vnTpO0kaSDgEXAZyVdLGmj+t+KiIgZDMHyubMl9COBn9peCLwF2AN4A8Xmzw+k2GJuPvBp4GDbD6fol/9/s9T7TuAM2w8Dvghs3/fczsBHyuduAp5n+4vAMuBFthfa/vPkCiUtlrRM0rJZXjsiYh2YiVUTlY62rO0GF+fbvs72BHAxsAPwEOAa2z8uyxwHPH6Weh4LfAHA9reA/m3lrrF9cfl4efkas7K9xPYi24uqlI+IWFtdb6Gv7SiX2/ser6rw83ey+kNj/kwFZ3iNdK9EROuGYbXF2VrotwKbzVLmSmAHSQ8qz18CnF0+/hmwV/n4eX0/cy7wAgBJTwXuWSHWKrFERDSn48NcZkzotm8Azi2HEr53mjK3AYcDJ0m6FJgAjimf/mfgA2W/9qq+H/tn4Kllvc8Hfk2RsGfyaeCY3BSNiLZ4otrRllm7XGwfOs311/Y9Pp3ihunkMt9l6v1Abwb+yvadkvYF9rZ9O0WLfre+n39f3+OTgZNnizcioild73Jpa6bo9sCJktYD7gCOaCmOiIhqbCaywcXd2f4JU7ToIyK6ahhWW1zbYYsREePJxSbRVY4qJB0g6UpJV0k6cpoyL5B0eTnR8nOz1ZnFuSIiqqqphS5pHvAR4CnAdcAFkpbavryvzM7APwCPsX2jpHvPVm9a6BERlVSbVFSxW2Yf4CrbV9u+g2Ki5YGTyhxBMWv+RgDbv52t0rTQR9Qr//bfG6n3yH8/ZvZC6+Dd//CqRuqNqNNE9T1FF0xahmSJ7SV959sA1/adXwc8clIdDwaQdC4wDziqnFk/rST0iIgKXPahV7SyhmVI1qdY22p/YFvgHEkPt33TdD+QLpeIiIpq7HK5Htiu73zb8lq/64Cltv9i+xrgxxQJflpJ6BERFdWY0C8Adpa0o6QNgRcCSyeV+QpF65xyefEHA1fPVGm6XCIiKqlvJcVylvxrgVMp+sePtb1C0tHAMttLy+eeKulyiqVT3lIuxzKtJPSIiCpqXm3R9inAKZOuvaPvsYE3lUclSegRERUY8KpuzxRNQo+IqGjsp/5L+kNN9SyU9PQ66oqIWGsVb4h2eU/RLlkIJKFHRGvqXMulCbUmdElfkbS8XEhmcd/195fXTpe0dXltoaTzJF0i6cuS7lleP0vSovLxAkk/K4f1HA0cXG5wcXCdcUdEVDFuLfSX2d4LWAS8XtJWwCYUw3AeRrE13TvLsp8B/t727sClfdfvplzr4B3ACbYX2j5hchlJiyUtmzTdNiKiFr3lc7uc0Ou+Kfp6Sc8pH29HMatpAugl4P8FviRpC2BL2729R48DThrkhct1EpYASOr2nYuIGD42HpcNLiTtDzwZ2Nf2nySdBcyfouhsyfZOVn9zmOrnIyJa0eZ+oVXU2eWyBXBjmcx3AR7V9xoHlY8PBf7P9s3AjZIeV15/CUV3DBT7iu5VPu79HBSbSG9WY7wREWul610udSb0bwHrS/oR8G7gvPL6H4F9JF0GPJHi5ibAS4H3SrqEYgRL7/r7gP8n6SJgQV/9ZwK75qZoRLTC3U/otXW52L4deNoUT206TfmLWd2K779+BbB736W3ldd/D+w9eKQREWtvGPYUzUzRiIhKzMSqbneiJ6FHRFRR8+JcTUhCj4ioKgk9ImI0dDyfJ6FHRFSRm6LRmiUf/Me2Q1grUv3rxLmhWSB3rlrVSL3rz5vXSL1Rk7XbJLoVSegREZWYiXGZ+h8RMerS5RIRMSqS0CMihp/Thx4RMTo63kBPQo+IqKbdhbeqSEKPiKjCZJRLRMQoMOlDj4gYGV3vcql/el5DJH1F0nJJKyQtbjueiBg3Loe6VDhaMkwt9JfZ/r2kjYALJJ1s+4bek2WST6KPiGZk+dxavV7Sc8rH2wE7A3cldNtLgCUAkrr9rkfEUJpY1e3UMhQJXdL+wJOBfctNqM8C5rcaVESMlay2WJ8tgBvLZL4LU+xFGhHRqHS51OZbwKsk/Qi4Ejiv5XgiYuxkYlEtbN8OPK3tOCJivCWhR0SMiEwsiogYAcOw2uLQTCyKiGib7UpHFZIOkHSlpKskHTlDuedJsqRFs9WZhB4RUUm1ZF4loUuaB3yE4t7grsAhknadotxmwN8CP6gSYRJ6REQVZZdLlaOCfYCrbF9t+w7gC8CBU5T7F+A9wG1VKk0fenSC3e0HwtXGAAAJaUlEQVRlSfutP29eI/U2NYJCUiP1jqO1+DtaIGlZ3/mScjZ7zzbAtX3n1wGP7K9A0p7Adra/IektVV40CT0iooK1nCm60vasfd7TkbQe8F/AYWvzc0noERGVGNe3wcX1FGtS9WxbXuvZDNgNOKv8hnVfYKmkZ9vub/mvIQk9IqIKQ409gxcAO0vakSKRvxA49K6Xsm8GFvTOy/Wr/m6mZA65KRoRUVldo1xs3wm8FjgV+BFwou0Vko6W9Ox1jS8t9IiIiuq8cW37FOCUSdfeMU3Z/avUmYQeEVHBMCyfO1RdLpJ2kHRZ23FExBiymVg1UeloS1roERFVdbyF3mhCl/R24MXA7ygG0S8HvgMcA2wM/JRir9AbJS2c5vpewLFllac1GW9ExExMtxN6Y10ukvYGngc8gmK9gt4g+88Af297d+BS4J2zXP8U8Drbj2gq1oiI2dj1Ls7VhCb70B8DfNX2bbZvBb4GbAJsafvsssxxwOMlbTHN9S3L6+eU14+f7sUkLZa0bNJ024iImhh7otLRlpHpQy/XSVgCIKnb34siYiiN8yiXc4FnSZovaVPgmcAfgRslPa4s8xLg7HJW1FTXbwJukvTY8vqLGow3ImJGExMTlY62NNZCt32BpKXAJcBvKPrFbwZeChwjaWPgauDw8kemu344cGzZ6s5N0YhoRdE/3u1VQZvucnmf7aPKJH0OsNz2xcCjJhec4fpyihurPW9tKtiIiBl1vMul6YS+pNyFYz5wnO0LG369iIjGdH3YYqMJ3fahs5eKiBgOXb8pOjKjXCIimmUmJla1HcSMktAjIiroTSzqsiT0iIiKktAjIkZEEnpEVFLuHVm7JpJQU7F2m8d+2GJExMgw4z2xKCJiJNi0Oq2/iiT0iIhK2l0at4ok9IiIisZ9LZeIiJGRFnpExIjoekJvcj30yiR9b4bn9pf09bmMJyLibuzqR0s60UK3/ei2Y4iImImBCXd7LZeutND/oMJ7JV0m6VJJB/cV2VzSNyRdKekYSZ2IOyLGSbUNotvslulEC730XGAhxWYWC4ALJPU2h94H2BX4OfCtsuwX+39Y0mJg8ZxFGxFjJ33o1T0W+LztVbZ/A5wN7F0+d77tq22vAj5fll2D7SW2F9leNHchR8Q4SQu9HpPfoW5/TEbEyCnud3Z7HHqXWujfBQ6WNE/S1sDjgfPL5/aRtGPZd34w8H9tBRkR48p4YqLS0ZautNANfBnYF/hhef5W27+WtAtwAfBh4EHAmWXZiIg5NdZ7ilYhaSvg9y46nt5SHnexfRZFaz0iolVdvynaakKXdH/gLOB9bcYRETE7d74PvdWEbvuXwIPbjCEiooph2FO0SzdFIyI6rc5hi5IOKCdLXiXpyCmef5OkyyVdIul0SQ+Yrc4k9IiIiiYmJiods5E0D/gI8DSKSZOHSNp1UrGLgEW2d6eYSPkfs9WbhB4RUYnBE9WO2e0DXFVOmLwD+AJw4BqvZp9p+0/l6XnAtrNV2vool4ho1lZb3b/2On+xcmXtdQJsv2BBI/XWZS2GLS6QtKzvfIntJX3n2wDX9p1fBzxyhvpeDnxzthdNQo+IqGAtb4qurGsZEkkvBhYB+81WNgk9IqKiGke5XA9s13e+bXltDZKeDPwTsJ/t22erNAk9IqKSWsehXwDsLGlHikT+QuDQ/gKS9gA+Bhxg+7dVKk1Cj4ioqMoIlips3ynptcCpwDzgWNsrJB0NLLO9FHgvsClwkiSAX9h+9kz1JqFHRFRQ98Qi26cAp0y69o6+x09e2zqT0CMiKml3v9AqktAjIioyWcslImIkdH0tlyT0iIhKXNtN0aYkoUdEVDAMW9CNTEKXtBhY3HYcETG60uUyR8p1EpYASOr2ux4RQykJPSJiJGTYYkTEyOj6JtFDtx66pFPKvUgjIuaMDRMTqyodbRm6Frrtp7cdQ0SMo+rby7Vl6BJ6RERbktAjIkZEEnpExIjIxKKIiFHgDFuMiBgJBibSQo+INv3+97+qvc7tttqq9jqHQbpcIiJGQoYtRkSMjCT0iIgRUPeeok1IQo+IqMS4xWn9VSShR0RU1PXFuZLQIyIqSpdLRMSISEKfgaQXAjvZflebcUREzMZ258ehz+l66JI2lLRJ36WnAd+qWDYiolVFUp/9aMucJHRJD5X0n8CVwIPLawIWAhdK2k/SxeVxkaTNgHsCKyR9TNLecxFnRMRMJiYmKh1taSyhS9pE0uGS/g/4OHA5sLvti8oiewA/dPFx9nfAa2wvBB4H/Nn2b4CHAGcC7yoT/esl3Wua11ssaZmkZU39mSJizPUW6JrtaIma+nog6RbgEuAVtq+Y4vl/BK6x/XlJRwLPAT4LfMn2dVOU3x74MPBU4IG2fznDa3f7zkXEkGswbzRSL7Dc9qJBKpg3b57nz6/WC/ynP9068Outiya7XA4Crge+JOkdkh4w6fmnAqcB2H438ApgI+BcSbv0Ckm6t6Q3A18D5gGHAr9pMO6IiLvpzRTtch96Y6NcbJ8GnCZpK+DFwFclraRI3DcC69u+AUDSTrYvBS4t+8t3kfQr4DhgF+B44Om2r28q3oiI2Yz9sMUyaX8A+ICkfYBVwFOA7/QVe4OkJwATwArgm8B84IPAme76uxgRY6HrqWhOx6HbPh9A0juBT/Rdf90UxW8Hzpij0CIiZmEmspbL3dl+RRuvGxGxrrLaYkTEKOl4Qp/TmaIREcPLlX9VIekASVdKuqocuj35+XtIOqF8/geSdpitziT0iIiK7IlKx2wkzQM+QrH8ya7AIZJ2nVTs5cCNth8EvB94z2z1JqFHRFRU49T/fYCrbF9t+w7gC8CBk8ocSDF0G+CLwJM0y8yrUe1DXwn8vGLZBWX5uqXe4Yp12OptNdZ1mNHZ9ns7eWLjuji1fL0q5k9ahmSJ7SV959sA1/adXwc8clIdd5Wxfaekm4GtmOHPO5IJ3fbWVctKWtbEFN3UO1yxDlu9wxTrMNY7FdsHzMXrDCJdLhERc+96YLu+823La1OWkbQ+sAVww0yVJqFHRMy9C4CdJe0oaUPghcDSSWWWAi8tHx8EnDHbrPmR7HJZS0tmL5J6O1Rn6m2uztQ7R8o+8ddS9MvPA461vULS0cAy20uBTwLHS7oK+D1F0p9RY8vnRkTE3EqXS0TEiEhCj4gYEUnoEREjIgk9ImJEJKFHRIyIJPSIiBGRhB4RMSL+PyKT93tRvA1NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_showattns(test, settings.MAX_LEN, SRC, TRG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.Adadelta()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
