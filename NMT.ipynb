{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"NMT by Jointly Learning to Align and Translate\" Implementation\n",
    "\n",
    "original paper: https://arxiv.org/abs/1409.0473"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references\n",
    "* arichitecture picture: https://arxiv.org/pdf/1703.03906.pdf\n",
    "* tutorial: https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "* data source: http://www.statmt.org/wmt14/translation-task.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field, Iterator, BucketIterator, TabularDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "# others\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 50000 samples for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'en_de_50000_train.txt'\n",
    "valid_path = 'en_de_50000_valid.txt'\n",
    "test_path = 'en_de_50000_test.txt'\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = Field(tokenize=str.split, use_vocab=True, init_token=\"<s>\", eos_token=\"</s>\", lower=True, \n",
    "               include_lengths=True, batch_first=True)\n",
    "TARGET = Field(tokenize=str.split, use_vocab=True, init_token=\"<s>\", eos_token=\"</s>\", lower=True, \n",
    "               batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = \\\n",
    "    TabularDataset.splits(path='data/en_de/', train=train_path, validation=valid_path, test=test_path, \n",
    "                          format='tsv', fields=[('so',SOURCE), ('ta',TARGET)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE.build_vocab(train_data)\n",
    "TARGET.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = BucketIterator(train_data, batch_size=BATCH_SIZE, device=DEVICE,\n",
    "                              sort_key=lambda x: len(x.so), sort_within_batch=True, repeat=False)\n",
    "valid_loader = BucketIterator(valid_data, batch_size=BATCH_SIZE, device=DEVICE,\n",
    "                              sort_key=lambda x: len(x.so), sort_within_batch=True, repeat=False)\n",
    "test_loader = BucketIterator(test_data, batch_size=BATCH_SIZE, device=DEVICE,\n",
    "                              sort_key=lambda x: len(x.so), sort_within_batch=True, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOURCE.vocab.itos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2,      5,   2729,  ...,    881,      6,      3],\n",
       "        [     2,     12,     24,  ...,    636,      6,      3],\n",
       "        [     2,     14,     56,  ...,     24,      6,      3],\n",
       "        ...,\n",
       "        [     2,     12,   2321,  ...,      6,      3,      1],\n",
       "        [     2,     94,    147,  ...,      6,      3,      1],\n",
       "        [     2,    166,    435,  ...,      6,      3,      1]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.so[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/encoder_decoder_att.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_so = len(SOURCE.vocab)\n",
    "V_ta = len(TARGET.vocab)\n",
    "E = 10\n",
    "H = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57486, 26356)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_so, V_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(V_so, E).cuda()\n",
    "rnn = nn.GRU(E, H, 3, batch_first=True, bidirectional=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, lengths = batch.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded = embed(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = pack_padded_sequence(embeded, lengths.tolist(), batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, hiddens = rnn(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.2349,  0.0056, -0.1244,  ...,  0.0629,  0.6755,  0.1037],\n",
       "        [ 0.1816, -0.0245, -0.0812,  ...,  0.1382,  0.6588,  0.1471],\n",
       "        [ 0.2105,  0.0483, -0.1101,  ...,  0.1284,  0.6618,  0.1982],\n",
       "        ...,\n",
       "        [ 0.5814,  0.3795,  0.0413,  ...,  0.0361,  0.2213,  0.0752],\n",
       "        [ 0.5160,  0.3953,  0.0727,  ..., -0.0105,  0.2171,  0.0390],\n",
       "        [ 0.4180,  0.3196, -0.0016,  ..., -0.0265,  0.2129,  0.0106]], device='cuda:0'), batch_sizes=tensor([ 32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  32,\n",
       "         32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  32,\n",
       "         32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  27]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, output_lengths = pad_packed_sequence(outputs, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35, 14])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 14])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens = torch.cat([h for h in hiddens[-2:]], 1).unsqueeze(1)\n",
    "hiddens.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed2 = nn.Embedding(V_ta, E).cuda()\n",
    "start = torch.LongTensor([0]*BATCH_SIZE).unsqueeze(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded2 = embed2(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 10]), torch.Size([32, 1, 14]), torch.Size([32, 35, 14]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded2.size(), hiddens.size(), outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_H = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper version\n",
    "attn_cat = nn.Linear(2*2*H, new_H).cuda()\n",
    "v_a = nn.Parameter(torch.FloatTensor(1, new_H)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35, 28])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.cat([hiddens.repeat(1, outputs.size(1), 1), outputs], 2)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 35, 8]), torch.Size([1, 8]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_cat(a).size(), v_a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 8])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v_a = v_a.repeat(outputs.size(0), 1).unsqueeze(1)\n",
    "new_v_a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 35])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result: B, 1, T_x\n",
    "x = new_v_a.bmm(attn_cat(a).transpose(1, 2))\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 35])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking\n",
    "mask = x.data.new(outputs.size(0), hiddens.size(1), outputs.size(1))\n",
    "mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4154, -0.2518, -0.1025, -0.0876,  0.1673,  0.6210,  0.2234,\n",
       "          0.5909,  0.3567, -0.0082,  0.3077, -0.2060,  0.1237, -0.1775,\n",
       "         -0.4154, -0.2518, -0.1025, -0.0876,  0.1673,  0.6210,  0.2234,\n",
       "          0.5909,  0.3567, -0.0082,  0.3077, -0.2060,  0.1237, -0.1775,\n",
       "         -0.4154, -0.2518, -0.1025, -0.0876,  0.1673,  0.6210,  0.2234]], device='cuda:0')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_unmask = 0\n",
    "v_mask = float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = output_lengths.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.fill_(v_unmask)\n",
    "n_context = mask.size(2)\n",
    "for i, size in enumerate(sizes):\n",
    "    if size < n_context:\n",
    "        mask[i,:,size:] = v_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0., -inf.]], device='cuda:0')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x += mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.2128e+14,  8.6409e+14,  7.5001e+14,  6.7749e+14,  7.3942e+14,\n",
       "          7.8272e+14,  7.5244e+14,  6.5769e+14,  5.5612e+14,  4.6153e+14,\n",
       "          3.7618e+14,  3.1627e+14,  2.4388e+14,  2.7045e+14,  2.9572e+14,\n",
       "          3.3994e+14,  4.4921e+14,  4.3587e+14,  4.5809e+14,  4.3460e+14,\n",
       "          4.4689e+14,  4.6073e+14,  4.3762e+14,  3.5159e+14,  2.7960e+14,\n",
       "          2.0945e+14,  1.2590e+14,  6.7760e+13,  1.2335e+14,  2.7290e+14,\n",
       "          3.9272e+14,  4.5216e+14,  5.5895e+14,  5.8419e+14,        -inf]], device='cuda:0')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 35])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = F.softmax(x, 2)\n",
    "weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], device='cuda:0')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 14])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.bmm(outputs).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat version\n",
    "attn = nn.Linear(2*2*H, 2*H).cuda()\n",
    "v_a = nn.Parameter(torch.FloatTensor(1, 2*H)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35, 28])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.cat([hiddens.repeat(1, outputs.size(1), 1), outputs], 2)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 14])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_a.repeat(outputs.size(0), 1).unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35, 14])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(a).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35, 14])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.tanh(attn(a)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 35])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_a.repeat(outputs.size(0), 1).unsqueeze(1).bmm(attn(a).transpose(1,2)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general version\n",
    "attn = nn.Linear(2*H, 2*H).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 35, 14]), torch.Size([32, 1, 14]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size(), hiddens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35, 14])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = attn(outputs)\n",
    "e.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 35])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens.bmm(outputs.transpose(1, 2)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 35])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot\n",
    "hiddens.bmm(outputs.transpose(1, 2)).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, V_e, m_e, n_e, num_layers=1, bidrec=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"\"\"\n",
    "        vocab_size: V_e\n",
    "        embed_size: m_e\n",
    "        hidden_size: n_e\n",
    "        \"\"\"\n",
    "        self.V_e = V_e\n",
    "        self.m_e = m_e\n",
    "        self.n_e = n_e\n",
    "        self.num_layers = num_layers\n",
    "        self.bidrec = bidrec\n",
    "        self.n_direct = 2 if bidrec else 1\n",
    "        \n",
    "        self.embed = nn.Embedding(V_e, m_e) \n",
    "        self.gru = nn.GRU(m_e, n_e, num_layers, batch_first=True, bidirectional=bidrec)\n",
    "        \n",
    "    def forward(self, inputs, lengths):\n",
    "        \"\"\"\n",
    "        input: \n",
    "        - inputs: B, T_x\n",
    "        - lengths: actual max length of batches\n",
    "        output:\n",
    "        - outputs: B, T_x, n_e\n",
    "        \"\"\"\n",
    "        # embeded: (B, T_x, n_e)\n",
    "        embeded = self.embed(inputs) \n",
    "        # packed: (B*T_x, n_e)\n",
    "        packed = pack_padded_sequence(embeded, lengths.tolist(), batch_first=True) \n",
    "        # packed outputs: (B*T_x, 2*n_e)\n",
    "        # hidden: (num of layers*n_direct, B, 2*n_e)\n",
    "        outputs, hidden = self.gru(packed)\n",
    "        # unpacked outputs: (B, T_x, 2*n_e)\n",
    "        outputs, output_lengths = pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        # hidden bidirection: (num of layers*n_direct(0,1,2...last one), B, n_e)\n",
    "        # choosen last hidden: (B, 1, 2*n_e)\n",
    "        hidden = torch.cat((h for h in hidden[-self.n_direct:]), 1).unsqueeze(1)\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, hidden_size2=None, method='general'):\n",
    "        super(Attn, self).__init__()\n",
    "        \"\"\"\n",
    "        hidden_size: set hidden size same as decoder hidden size which is n_d (= 2*n_e)\n",
    "        hidden_size2: only for concat method, if none then is same as hidden_size (n_d)\n",
    "        (in paper notation is n', https://arxiv.org/abs/1409.0473)\n",
    "        methods:\n",
    "        - 'dot': dot product between hidden and encoder_outputs\n",
    "        - 'general': encoder_outputs through a linear layer \n",
    "        - 'concat': concat (hidden, encoder_outputs)\n",
    "        - 'paper': concat + tanh\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size \n",
    "        self.hidden_size2 = hidden_size2 if hidden_size2 else hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size) \n",
    "            # linear_weight shape: (out_f, in_f)\n",
    "            # linear input: (B, *, in_f)\n",
    "            # linear output: (B, *, out_f)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size*2, self.hidden_size2)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, self.hidden_size2))\n",
    "        \n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, encoder_lengths=None, return_weight=False):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        - hidden, previous hidden(= H): B, 1, n_d \n",
    "        - encoder_outputs, source context(= O): B, T_x, n_d\n",
    "        - encoder_lengths: real lengths of encoder outputs\n",
    "        - return_weight = return weights(alphas)\n",
    "        output:\n",
    "        - attentioned_hidden(= z): B, 1, n_d\n",
    "        - weights(= w): B, 1, T_x\n",
    "        \"\"\"\n",
    "        H, O = hidden, encoder_outputs\n",
    "        # Batch(B), Seq_length(T), dimemsion(n)\n",
    "        B_H, T_H, n_H = H.size()\n",
    "        B_O, T_O, n_O = O.size()\n",
    "        \n",
    "        if B_H != B_O:\n",
    "            msg = \"Batch size is not correct, H: {} O: {}\".format(H.size(), O.size())\n",
    "            raise ValueError(msg)\n",
    "        else:\n",
    "            B = B_H\n",
    "        \n",
    "        # score: (B, 1, T_x)\n",
    "        s = self.score(H, O) \n",
    "        \n",
    "        # encoding masking\n",
    "        if encoder_lengths is not None:\n",
    "            mask = s.data.new(B, T_H, T_O) # (B, 1, T_x)\n",
    "            mask = self.fill_context_mask(mask, sizes=encoder_lengths, v_mask=float('-inf'), v_unmask=0)\n",
    "            s += mask\n",
    "        \n",
    "        # softmax: (B, 1, T_x)\n",
    "        w = F.softmax(s, 2) \n",
    "        \n",
    "        # attention: weight * encoder_hiddens, (B, 1, T_x) * (B, T_x, n_d) = (B, 1, n_d)\n",
    "        z = w.bmm(c)\n",
    "        if return_weight:\n",
    "            return z, w\n",
    "        return z\n",
    "    \n",
    "    def score(self, H, O):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - hiddden, previous hidden(= H): B, 1, n_d \n",
    "        - encoder_outputs, source context(= O): B, T_x, n_d\n",
    "        \"\"\"\n",
    "        if self.method == 'dot':\n",
    "            # bmm: (B, 1, n_d) * (B, n_d, T_x) = (B, 1, T_x)\n",
    "            e = H.bmm(O.transpose(1, 2))\n",
    "            return e\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            # attn: (B, T_x, n_d) > (B, T_x, n_d)\n",
    "            # bmm: (B, 1, n_d) * (B, n_d, T_x) = (B, 1, T_x)\n",
    "            e = self.attn(O)\n",
    "            e = H.bmm(e.transpose(1, 2))\n",
    "            return e\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            # H repeat: (B, 1, n_d) > (B, T_x, n_d)\n",
    "            # cat: (B, T_x, 2*n_d)\n",
    "            # attn: (B, T_x, 2*n_d) > (B, T_x, n_d)\n",
    "            # v repeat: (1, n_d) > (B, 1, n_d)\n",
    "            # bmm: (B, 1, n_d) * (B, n_d, T_x) = (B, 1, T_x)\n",
    "            cat = torch.cat((H.repeat(1, O.size(1), 1), O), 2)\n",
    "            e = self.attn(cat)\n",
    "            v = self.v.repeat(O.size(0), 1).unsqueeze(1)\n",
    "            e = v.bmm(e.transpose(1, 2))\n",
    "            return e\n",
    "        \n",
    "        elif self.method == 'paper':\n",
    "            # add tanh after attention linear layer in 'concat' method\n",
    "            cat = torch.cat((H.repeat(1, O.size(1), 1), O), 2)\n",
    "            e = F.tanh(self.attn(cat))\n",
    "            v = self.v.repeat(O.size(0), 1).unsqueeze(1)\n",
    "            e = v.bmm(e.transpose(1, 2))\n",
    "            return e\n",
    "    \n",
    "    def fill_context_mask(self, mask, sizes, v_mask, v_unmask):\n",
    "        \"\"\"Fill attention mask inplace for a variable length context.\n",
    "        Args\n",
    "        ----\n",
    "        mask: Tensor of size (B, T, D)\n",
    "            Tensor to fill with mask values. \n",
    "        sizes: list[int]\n",
    "            List giving the size of the context for each item in\n",
    "            the batch. Positions beyond each size will be masked.\n",
    "        v_mask: float\n",
    "            Value to use for masked positions.\n",
    "        v_unmask: float\n",
    "            Value to use for unmasked positions.\n",
    "        Returns\n",
    "        -------\n",
    "        mask:\n",
    "            Filled with values in {v_mask, v_unmask}\n",
    "        \"\"\"\n",
    "        mask.fill_(v_unmask)\n",
    "        n_context = mask.size(2)\n",
    "        for i, size in enumerate(sizes):\n",
    "            if size < n_context:\n",
    "                mask[i,:,size:] = v_mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, V_d, m_d, n_d, sos_idx, num_layers=1, hidden_size2=None, method='general'):\n",
    "        super(Decoder, self).__init__()\n",
    "        \"\"\"\n",
    "        vocab_size: V_d\n",
    "        embed_size: m_d\n",
    "        hidden_size: n_d (set this value as 2*n_e)\n",
    "        methods:\n",
    "        - 'dot': dot product between hidden and encoder_outputs\n",
    "        - 'general': encoder_outputs through a linear layer \n",
    "        - 'concat': concat (hidden, encoder_outputs)\n",
    "        - 'paper': concat + tanh\n",
    "        \"\"\"\n",
    "        self.V_d = V_d\n",
    "        self.m_d = m_d\n",
    "        self.n_d = n_d\n",
    "        self.sos_idx = sos_idx\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.attention = Attention(hidden_size=n_d, hidden_size2=hidden_size2, method=method)\n",
    "        \n",
    "        self.embed = nn.Embedding(V_d, embed_size)\n",
    "        # gru(W*[embed, context] + U*[hidden_prev])\n",
    "        # gru: m+n\n",
    "        self.gru = nn.GRU(m_d+n_d, n_d, num_layers, batch_first=True, bidirectional=False) \n",
    "        \n",
    "    def start_token(self, batch_size):\n",
    "        sos = torch.LongTensor([self.sos_idx]*batch_size).unsqueeze(1)\n",
    "        if USE_CUDA: sos = sos.cuda()\n",
    "        return sos        \n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs, encoder_outputs_lengths, max_len):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        - hidden, previous hidden: B, 1, n_d \n",
    "        - encoder_outputs, source context: B, T_x, n_d\n",
    "        start_token: B, 1\n",
    "        \"\"\"\n",
    "        inputs = self.start_token(hidden.size(0)) # (B, 1)\n",
    "        embed = self.embed(inputs) # (B, 1, m_d)\n",
    "        \n",
    "        embed\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z,.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def read_files(path, max_len=None, min_len=None, n_sentences=None):\n",
    "    source = []\n",
    "    target = []\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for l in file.readlines():\n",
    "            so, ta = l.split('\\t')\n",
    "            normed_so = normalize_string(so.strip()).split()\n",
    "            normed_ta = normalize_string(ta.strip()).split() \n",
    "            if len(normed_so) >= min_len and len(normed_so) <= max_len and \\\n",
    "               len(normed_ta) >= min_len and len(normed_ta) <= max_len:\n",
    "                source.append(normed_so)\n",
    "                target.append(normed_ta)\n",
    "        if n_sentences:\n",
    "            source = source[:n_sentences]\n",
    "            target = target[:n_sentences]\n",
    "    return source, target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
