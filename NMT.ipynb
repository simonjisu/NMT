{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"NMT by Jointly Learning to Align and Translate\" Implementation\n",
    "\n",
    "original paper: https://arxiv.org/abs/1409.0473"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references\n",
    "* arichitecture picture: https://arxiv.org/pdf/1703.03906.pdf\n",
    "* tutorial: https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "* data source: http://www.statmt.org/wmt14/translation-task.html\n",
    "* data source2: http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field, Iterator, BucketIterator, TabularDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "# others\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.cuda.current_device()\n",
    "# USE_CUDA = False\n",
    "# DEVICE = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = 'data/en_de/en_de_1000.txt' # cannot use because of gpu memory is not enough\n",
    "train_path = 'data/en_fa/eng-fra.txt'\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = Field(tokenize=str.split, use_vocab=True, init_token=\"<s>\", eos_token=\"</s>\", lower=True, \n",
    "               include_lengths=True, batch_first=True)\n",
    "TARGET = Field(tokenize=str.split, use_vocab=True, init_token=\"<s>\", eos_token=\"</s>\", lower=True, \n",
    "               batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= TabularDataset(path=train_path, format='tsv', \n",
    "                           fields=[('so', SOURCE), ('ta', TARGET)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE.build_vocab(train_data)\n",
    "TARGET.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = BucketIterator(train_data, batch_size=BATCH_SIZE, device=DEVICE,\n",
    "                              sort_key=lambda x: len(x.so), sort_within_batch=True, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5902, 10240)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SOURCE.vocab), len(TARGET.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/encoder_decoder_att.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, V_e, m_e, n_e, num_layers=1, bidrec=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"\"\"\n",
    "        vocab_size: V_e\n",
    "        embed_size: m_e\n",
    "        hidden_size: n_e\n",
    "        \"\"\"\n",
    "        self.V_e = V_e\n",
    "        self.m_e = m_e\n",
    "        self.n_e = n_e\n",
    "        self.num_layers = num_layers\n",
    "        self.bidrec = bidrec\n",
    "        self.n_direct = 2 if bidrec else 1\n",
    "        \n",
    "        self.embed = nn.Embedding(V_e, m_e) \n",
    "        self.gru = nn.GRU(m_e, n_e, num_layers, batch_first=True, bidirectional=bidrec)\n",
    "        \n",
    "    def forward(self, inputs, lengths):\n",
    "        \"\"\"\n",
    "        input: \n",
    "        - inputs: B, T_x\n",
    "        - lengths: actual max length of batches\n",
    "        output:\n",
    "        - outputs: B, T_x, n_e\n",
    "        \"\"\"\n",
    "        # embeded: (B, T_x, n_e)\n",
    "        embeded = self.embed(inputs) \n",
    "        # packed: (B*T_x, n_e)\n",
    "        packed = pack_padded_sequence(embeded, lengths, batch_first=True) \n",
    "        # packed outputs: (B*T_x, 2*n_e)\n",
    "        # hidden: (num of layers*n_direct, B, 2*n_e)\n",
    "        outputs, hidden = self.gru(packed)\n",
    "        # unpacked outputs: (B, T_x, 2*n_e)\n",
    "        outputs, output_lengths = pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        # hidden bidirection: (num of layers*n_direct(0,1,2...last one), B, n_e)\n",
    "        # choosen last hidden: (B, 1, 2*n_e)\n",
    "        hidden = torch.cat([h for h in hidden[-self.n_direct:]], 1).unsqueeze(1)\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, hidden_size2=None, method='general'):\n",
    "        super(Attention, self).__init__()\n",
    "        \"\"\"\n",
    "        hidden_size: set hidden size same as decoder hidden size which is n_d (= 2*n_e)\n",
    "        hidden_size2: only for concat&paper method, if none then is same as hidden_size (n_d)\n",
    "        (in paper notation is n', https://arxiv.org/abs/1409.0473)\n",
    "        methods:\n",
    "        - 'dot': dot product between hidden and encoder_outputs\n",
    "        - 'general': encoder_outputs through a linear layer \n",
    "        - 'concat': concat (hidden, encoder_outputs)\n",
    "        - 'paper': concat + tanh\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size \n",
    "        self.hidden_size2 = hidden_size2 if hidden_size2 else hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size) \n",
    "            # linear_weight shape: (out_f, in_f)\n",
    "            # linear input: (B, *, in_f)\n",
    "            # linear output: (B, *, out_f)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size*2, self.hidden_size2)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, self.hidden_size2))\n",
    "        \n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, encoder_lengths=None, return_weight=False):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        - hidden, previous hidden(= H): B, 1, n_d \n",
    "        - encoder_outputs, source context(= O): B, T_x, n_d\n",
    "        - encoder_lengths: real lengths of encoder outputs\n",
    "        - return_weight = return weights(alphas)\n",
    "        output:\n",
    "        - attentioned_hidden(= z): B, 1, n_d\n",
    "        - weights(= w): B, 1, T_x\n",
    "        \"\"\"\n",
    "        H, O = hidden, encoder_outputs\n",
    "        # Batch(B), Seq_length(T), dimemsion(n)\n",
    "        B_H, T_H, n_H = H.size()\n",
    "        B_O, T_O, n_O = O.size()\n",
    "        \n",
    "        if B_H != B_O:\n",
    "            msg = \"Batch size is not correct, H: {} O: {}\".format(H.size(), O.size())\n",
    "            raise ValueError(msg)\n",
    "        else:\n",
    "            B = B_H\n",
    "        \n",
    "        # score: (B, 1, T_x)\n",
    "        s = self.score(H, O) \n",
    "        \n",
    "        # encoding masking: ensure not to calculate paddings\n",
    "        if encoder_lengths is not None:\n",
    "            mask = s.data.new(B, T_H, T_O) # (B, 1, T_x)\n",
    "            mask = self.fill_context_mask(mask, sizes=encoder_lengths, v_mask=float('-inf'), v_unmask=0)\n",
    "            s += mask\n",
    "        \n",
    "        # softmax: (B, 1, T_x)\n",
    "        w = F.softmax(s, 2) \n",
    "        \n",
    "        # attention: weight * encoder_hiddens, (B, 1, T_x) * (B, T_x, n_d) = (B, 1, n_d)\n",
    "        z = w.bmm(O)\n",
    "        if return_weight:\n",
    "            return z, w\n",
    "        return z\n",
    "    \n",
    "    def score(self, H, O):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - hiddden, previous hidden(= H): B, 1, n_d \n",
    "        - encoder_outputs, source context(= O): B, T_x, n_d\n",
    "        \"\"\"\n",
    "        if self.method == 'dot':\n",
    "            # bmm: (B, 1, n_d) * (B, n_d, T_x) = (B, 1, T_x)\n",
    "            e = H.bmm(O.transpose(1, 2))\n",
    "            return e\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            # attn: (B, T_x, n_d) > (B, T_x, n_d)\n",
    "            # bmm: (B, 1, n_d) * (B, n_d, T_x) = (B, 1, T_x)\n",
    "            e = self.attn(O)\n",
    "            e = H.bmm(e.transpose(1, 2))\n",
    "            return e\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            # H repeat: (B, 1, n_d) > (B, T_x, n_d)\n",
    "            # cat: (B, T_x, 2*n_d)\n",
    "            # attn: (B, T_x, 2*n_d) > (B, T_x, n_d)\n",
    "            # v repeat: (1, n_d) > (B, 1, n_d)\n",
    "            # bmm: (B, 1, n_d) * (B, n_d, T_x) = (B, 1, T_x)\n",
    "            cat = torch.cat([H.repeat(1, O.size(1), 1), O], 2)\n",
    "            e = self.attn(cat)\n",
    "            v = self.v.repeat(O.size(0), 1).unsqueeze(1)\n",
    "            e = v.bmm(e.transpose(1, 2))\n",
    "            return e\n",
    "        \n",
    "        elif self.method == 'paper':\n",
    "            # add tanh after attention linear layer in 'concat' method\n",
    "            cat = torch.cat([H.repeat(1, O.size(1), 1), O], 2)\n",
    "            e = F.tanh(self.attn(cat))\n",
    "            v = self.v.repeat(O.size(0), 1).unsqueeze(1)\n",
    "            e = v.bmm(e.transpose(1, 2))\n",
    "            return e\n",
    "    \n",
    "    def fill_context_mask(self, mask, sizes, v_mask, v_unmask):\n",
    "        \"\"\"Fill attention mask inplace for a variable length context.\n",
    "        Args\n",
    "        ----\n",
    "        mask: Tensor of size (B, T, D)\n",
    "            Tensor to fill with mask values. \n",
    "        sizes: list[int]\n",
    "            List giving the size of the context for each item in\n",
    "            the batch. Positions beyond each size will be masked.\n",
    "        v_mask: float\n",
    "            Value to use for masked positions.\n",
    "        v_unmask: float\n",
    "            Value to use for unmasked positions.\n",
    "        Returns\n",
    "        -------\n",
    "        mask:\n",
    "            Filled with values in {v_mask, v_unmask}\n",
    "        \"\"\"\n",
    "        mask.fill_(v_unmask)\n",
    "        n_context = mask.size(2)\n",
    "        for i, size in enumerate(sizes):\n",
    "            if size < n_context:\n",
    "                mask[i,:,size:] = v_mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, V_d, m_d, n_d, sos_idx, num_layers=1, hidden_size2=None, method='general', \n",
    "                 return_weight=True, max_len=15):\n",
    "        super(Decoder, self).__init__()\n",
    "        \"\"\"\n",
    "        vocab_size: V_d\n",
    "        embed_size: m_d\n",
    "        hidden_size: n_d (set this value as 2*n_e)\n",
    "        methods:\n",
    "        - 'dot': dot product between hidden and encoder_outputs\n",
    "        - 'general': encoder_outputs through a linear layer \n",
    "        - 'concat': concat (hidden, encoder_outputs)\n",
    "        - 'paper': concat + tanh\n",
    "        return_weight: return attention weights\n",
    "        \"\"\"\n",
    "        self.V_d = V_d\n",
    "        self.m_d = m_d\n",
    "        self.n_d = n_d\n",
    "        self.sos_idx = sos_idx\n",
    "        self.num_layers = num_layers\n",
    "        self.return_weight = return_weight\n",
    "        self.method = method\n",
    "        # attention\n",
    "        self.attention = Attention(hidden_size=n_d, hidden_size2=hidden_size2, method=method)\n",
    "        # embed\n",
    "        self.embed = nn.Embedding(V_d, m_d)\n",
    "        # gru(W*[embed, context] + U*[hidden_prev])\n",
    "        # gru: m+n\n",
    "        self.gru = nn.GRU(m_d+n_d, n_d, num_layers, batch_first=True, bidirectional=False) \n",
    "        # linear\n",
    "        self.linear = nn.Linear(2*n_d, V_d)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        \n",
    "    def start_token(self, batch_size):\n",
    "        sos = torch.LongTensor([self.sos_idx]*batch_size).unsqueeze(1)\n",
    "        if USE_CUDA: sos = sos.cuda()\n",
    "        return sos\n",
    "    \n",
    "    def forward(self, hidden, enc_outputs, enc_outputs_lengths=None, max_len=None):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        - hidden(previous hidden): B, 1, n_d \n",
    "        - enc_outputs(source context): B, T_x, n_d\n",
    "        - enc_outputs_lengths: list type\n",
    "        - max_len(targer sentences max len in batch): T_y\n",
    "        \"\"\"\n",
    "        if max_len is None: max_len = self.max_len\n",
    "        \n",
    "        inputs = self.start_token(hidden.size(0)) # (B, 1)\n",
    "        embeded = self.embed(inputs) # (B, 1, m_d)\n",
    "        # prepare for whole targer sentence scores\n",
    "        scores = []\n",
    "        attn_weights = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            # context vector: previous hidden(s{i-1}), encoder_outputs(O_e) > context(c{i}), weights\n",
    "            # - context: (B, 1, n_d)\n",
    "            # - weights: (B, 1, T_x)\n",
    "            context, weights = self.attention(hidden, enc_outputs, enc_outputs_lengths, \n",
    "                                              return_weight=self.return_weight)\n",
    "            attn_weights.append(weights.squeeze(1))\n",
    "            \n",
    "            # concat context & embedding vectors: (B, 1, m_d+n_d)\n",
    "            gru_input = torch.cat([embeded, context], 2)\n",
    "            \n",
    "            # gru((context&embedding), previous hidden)\n",
    "            # output hidden(s{i}): (1, B, n_d)\n",
    "            _, hidden = self.gru(gru_input, hidden.transpose(0, 1))\n",
    "            hidden = hidden.transpose(0, 1) # change shape to (B, 1, n_d) again\n",
    "            \n",
    "            # concat context and new hidden vectors: (B, 1, 2*n_d)\n",
    "            concated = torch.cat([hidden, context], 2)\n",
    "            \n",
    "            # get score: (B, V_d)\n",
    "            score = self.linear(concated.squeeze(1))\n",
    "            scores.append(score)\n",
    "            \n",
    "            # greedy method\n",
    "            decoded = score.max(1)[1]  # (B)\n",
    "            embeded = self.embed(decoded).unsqueeze(1) # next input y{i-1} (B, 1, m_d) \n",
    "\n",
    "        # column-wise concat, reshape!! \n",
    "        # scores = [(B, V_d), (B, V_d), (B, V_d)...] > (B, V_d*max_len)\n",
    "        # attn_weights = [(B, T_x), (B, T_x), (B, T_x)...] > (B*max_len, T_x)\n",
    "        scores = torch.cat(scores, 1)\n",
    "        return scores.view(inputs.size(0)*max_len, -1), torch.cat(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_so = len(SOURCE.vocab)\n",
    "V_ta = len(TARGET.vocab)\n",
    "HIDDEN = 300\n",
    "EMBED = 300\n",
    "STEP = 200\n",
    "LR = 0.001\n",
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(V_so, EMBED, HIDDEN, NUM_LAYERS, bidrec=True)\n",
    "dec = Decoder(V_ta, EMBED, 2*HIDDEN, sos_idx=SOURCE.vocab.stoi['<s>'], method='general')\n",
    "if USE_CUDA:\n",
    "    enc = enc.cuda()\n",
    "    dec = dec.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=TARGET.vocab.stoi['<pad>'])\n",
    "optimizer = optim.Adam(list(enc.parameters()) + list(dec.parameters()), lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.1, milestones=[100], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200] train_loss: 2.8617, lr: 0.001\n",
      "[11/200] train_loss: 0.6985, lr: 0.001\n",
      "[21/200] train_loss: 0.6035, lr: 0.001\n",
      "[31/200] train_loss: 0.5674, lr: 0.001\n",
      "[41/200] train_loss: 0.5470, lr: 0.001\n",
      "[51/200] train_loss: 0.5316, lr: 0.001\n",
      "[61/200] train_loss: 0.5174, lr: 0.001\n",
      "[71/200] train_loss: 0.5020, lr: 0.001\n",
      "[81/200] train_loss: 0.5059, lr: 0.001\n",
      "[91/200] train_loss: 0.4911, lr: 0.001\n",
      "[101/200] train_loss: 0.3833, lr: 0.0001\n",
      "[111/200] train_loss: 0.2433, lr: 0.0001\n",
      "[121/200] train_loss: 0.2308, lr: 0.0001\n",
      "[131/200] train_loss: 0.2261, lr: 0.0001\n",
      "[141/200] train_loss: 0.2210, lr: 0.0001\n",
      "[151/200] train_loss: 0.2203, lr: 0.0001\n",
      "[161/200] train_loss: 0.2179, lr: 0.0001\n",
      "[171/200] train_loss: 0.2161, lr: 0.0001\n",
      "[181/200] train_loss: 0.2149, lr: 0.0001\n",
      "[191/200] train_loss: 0.2131, lr: 0.0001\n"
     ]
    }
   ],
   "source": [
    "enc.train()\n",
    "dec.train()\n",
    "for step in range(STEP):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, lengths = batch.so\n",
    "        targets = batch.ta\n",
    "        \n",
    "        enc.zero_grad()\n",
    "        dec.zero_grad()\n",
    "        \n",
    "        output, hidden = enc(inputs, lengths.tolist())\n",
    "        preds, _ = dec(hidden, output, lengths.tolist(), targets.size(1)) # max_len\n",
    "        \n",
    "        loss = loss_function(preds, targets.view(-1))\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        msg = '[{}/{}] train_loss: {:.4f}, lr: {}'.format(\\\n",
    "          step+1, STEP, np.mean(losses), round(scheduler.get_lr()[0], 6))\n",
    "        print(msg)\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc.state_dict(), './data//model/fra_eng.enc')\n",
    "torch.save(dec.state_dict(), './data/model/fra_eng.dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(V_so, EMBED, HIDDEN, NUM_LAYERS, bidrec=True)\n",
    "dec = Decoder(V_ta, EMBED, 2*HIDDEN, sos_idx=SOURCE.vocab.stoi['<s>'], method='general')\n",
    "if USE_CUDA:\n",
    "    enc = enc.cuda()\n",
    "    dec = dec.cuda()\n",
    "    \n",
    "enc.load_state_dict(torch.load('./data//model/fra_eng.enc'))\n",
    "dec.load_state_dict(torch.load('./data//model/fra_eng.dec'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test & Visualize Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed code from https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "\n",
    "def show_attention(input_words, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_words, rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentence(x):\n",
    "    return [i for i in x if i not in ['<s>', '</s>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source :  i was born in osaka .\n",
      "Truth :  je suis nee a osaka .\n",
      "Prediction :  je suis nee d osaka . .\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc.eval()\n",
    "dec.eval()\n",
    "\n",
    "test = random.choice(train_data)\n",
    "source_sentence = test.so\n",
    "target_sentence = test.ta\n",
    "\n",
    "s = torch.LongTensor(list(map(lambda x: SOURCE.vocab.stoi[x], source_sentence))).view(1, -1)\n",
    "t = torch.LongTensor(list(map(lambda x: TARGET.vocab.stoi[x], target_sentence))).view(1, -1)\n",
    "if USE_CUDA:\n",
    "    s = s.cuda()\n",
    "    t = t.cuda()\n",
    "\n",
    "output, hidden = enc(s, [s.size(1)])\n",
    "pred, attn = dec(hidden, output)\n",
    "pred_sentence = [TARGET.vocab.itos[i] for i in pred.max(1)[1]]\n",
    "\n",
    "\n",
    "print('Source : ', ' '.join(print_sentence(source_sentence)))\n",
    "print('Truth : ', ' '.join(print_sentence(target_sentence)))\n",
    "print('Prediction : ', ' '.join(print_sentence(pred_sentence)))\n",
    "\n",
    "if USE_CUDA:\n",
    "    attn = attn.cpu()\n",
    "\n",
    "show_attention(source_sentence, pred_sentence, attn.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
