{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"NMT by Jointly Learning to Align and Translate\" Implementation\n",
    "\n",
    "original paper: https://arxiv.org/abs/1409.0473"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references\n",
    "* arichitecture picture: https://arxiv.org/pdf/1703.03906.pdf\n",
    "* tutorial: https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "* data source: http://www.statmt.org/wmt14/translation-task.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field, Iterator, BucketIterator, TabularDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "# others\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bring code from tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z,.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path, max_len=None, min_len=None, n_sentences=None):\n",
    "    source = []\n",
    "    target = []\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for l in file.readlines():\n",
    "            so, ta = l.split('\\t')\n",
    "            normed_so = normalize_string(so.strip()).split()\n",
    "            normed_ta = normalize_string(ta.strip()).split() \n",
    "            if len(normed_so) >= min_len and len(normed_so) <= max_len and \\\n",
    "               len(normed_ta) >= min_len and len(normed_ta) <= max_len:\n",
    "                source.append(normed_so)\n",
    "                target.append(normed_ta)\n",
    "        if n_sentences:\n",
    "            source = source[:n_sentences]\n",
    "            target = target[:n_sentences]\n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(path, source, target):\n",
    "    with open(path, 'w', encoding='utf-8') as file:\n",
    "        for s, t in zip(source, target):\n",
    "            print(' '.join(s) + '\\t' + ' '.join(t), file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154883\n"
     ]
    }
   ],
   "source": [
    "corpus = open('./data/en_fa/fra.txt', 'r', encoding='utf-8')\n",
    "corpus_len = len(corpus.readlines())\n",
    "print(corpus_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 25\n",
    "MIN_LEN = 3\n",
    "N_SENTENCES = 50000\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target = read_files('./data/en_fa/fra.txt', max_len=MAX_LEN, min_len=MIN_LEN, n_sentences=N_SENTENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file('./data/en_fa/eng-fra.txt', source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = Field(tokenize=str.split, use_vocab=True, init_token=\"<s>\", eos_token=\"</s>\", lower=True, \n",
    "               include_lengths=True, batch_first=True)\n",
    "TARGET = Field(tokenize=str.split, use_vocab=True, init_token=\"<s>\", eos_token=\"</s>\", lower=True, \n",
    "               batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TabularDataset(path='./data/en_fa/eng-fra.txt', format='tsv',\n",
    "                           fields=[('inputs',SOURCE), ('targets',TARGET)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE.build_vocab(train_data)\n",
    "TARGET.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = BucketIterator(train_data, batch_size=BATCH_SIZE, device=DEVICE,\n",
    "                              sort_key=lambda x: len(x.inputs), sort_within_batch=True, repeat=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/encoder_decoder_att.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_so = len(SOURCE.vocab)\n",
    "V_ta = len(TARGET.vocab)\n",
    "E = 10\n",
    "H = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5902, 10240)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_so, V_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(V_so, E).cuda()\n",
    "rnn = nn.GRU(E, H, 3, batch_first=True, bidirectional=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, lengths = batch.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded = embed(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = pack_padded_sequence(embeded, lengths.tolist(), batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 10])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hidden = rnn(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.3438, -0.2094,  0.0553,  ..., -0.0368,  0.3782, -0.0662],\n",
       "        [ 0.3376, -0.2095,  0.0582,  ..., -0.0385,  0.4570, -0.0491],\n",
       "        [ 0.3617, -0.2198,  0.0587,  ..., -0.1037,  0.3921, -0.0623],\n",
       "        ...,\n",
       "        [ 0.6185, -0.3715,  0.4607,  ...,  0.1606,  0.2110, -0.0422],\n",
       "        [ 0.6751, -0.3095,  0.4269,  ...,  0.1482,  0.2391, -0.0927],\n",
       "        [ 0.6642, -0.3478,  0.4251,  ...,  0.1590,  0.2273, -0.0820]], device='cuda:0'), batch_sizes=tensor([ 32,  32,  32,  32,  32,  32,  32,  32]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, output_lengths = pad_packed_sequence(output, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 10])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 32, 5])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3612, -0.2010,  0.0522,  0.1332,  0.1297,  0.1936,  0.1296,\n",
       "         -0.0584,  0.1640, -0.1198],\n",
       "        [ 0.5923, -0.3004,  0.1281,  0.2247,  0.1404,  0.2127,  0.1010,\n",
       "         -0.0364,  0.1256, -0.1407],\n",
       "        [ 0.6283, -0.3849,  0.1932,  0.2269,  0.2364,  0.2150,  0.1087,\n",
       "          0.0181,  0.0999, -0.0342],\n",
       "        [ 0.7000, -0.4010,  0.2364,  0.2917,  0.1795,  0.2200,  0.0970,\n",
       "          0.0169,  0.1096, -0.1239],\n",
       "        [ 0.7212, -0.4147,  0.2818,  0.3169,  0.1630,  0.2033,  0.0865,\n",
       "          0.0474,  0.2006, -0.1365],\n",
       "        [ 0.7277, -0.4069,  0.3254,  0.3664,  0.1024,  0.1463,  0.0473,\n",
       "          0.0985,  0.2934, -0.1801],\n",
       "        [ 0.7296, -0.3676,  0.3751,  0.4147,  0.0294,  0.1270,  0.0302,\n",
       "          0.1750,  0.3167, -0.2126],\n",
       "        [ 0.6642, -0.3478,  0.4251,  0.4276,  0.0675,  0.0753, -0.0093,\n",
       "          0.1590,  0.2273, -0.0820]], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([h for h in hidden[-2:]], 1).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, bidrec=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"\"\"\n",
    "        vocab_size: K\n",
    "        embed_size: m\n",
    "        hidden_size: n\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidrec = bidrec\n",
    "        self.n_direct = 2 if bidrec else 1\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_size) \n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True, bidirectional=bidrec)\n",
    "        \n",
    "    def forward(inputs):\n",
    "        \"\"\"\n",
    "        inputs: B, T_x\n",
    "        \"\"\"\n",
    "        # embeded: B, T_x, n\n",
    "        embeded = self.embed(inputs) \n",
    "        # packed: B x T_x, n\n",
    "        packed = pack_padded_sequence(embeded, lengths.tolist(), batch_first=True) \n",
    "        # outputs: B*T_x, n\n",
    "        # hidden: num of layers*n_direct, B, n \n",
    "        output, hidden = self.gru(packed)\n",
    "        # outputs: B, T_x, n\n",
    "        output, output_lengths = pad_packed_sequence(output, batch_first=True)\n",
    "        \n",
    "        # hidden bidirection: [num of layers*n_direct(0,1,2...last one), B, n) \n",
    "        hidden = torch.cat([h for h in hidden[-self.n_direct:]], 1).unsqueeze(0)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
