{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"NMT by Jointly Learning to Align and Translate\" Implementation\n",
    "\n",
    "original paper: https://arxiv.org/abs/1409.0473"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references\n",
    "* arichitecture picture: https://arxiv.org/pdf/1703.03906.pdf\n",
    "* tutorial: https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "* data source: http://www.statmt.org/wmt14/translation-task.html\n",
    "* data source2: http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field, Iterator, BucketIterator, TabularDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "# others\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.cuda.current_device()\n",
    "# USE_CUDA = False\n",
    "# DEVICE = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = 'data/en_de/en_de_1000.txt' # cannot use because of gpu memory is not enough\n",
    "train_file = 'eng-fra-filtered.train'\n",
    "valid_file = 'eng-fra-filtered.valid'\n",
    "test_file = 'eng-fra-filtered.test'\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = Field(tokenize=str.split, use_vocab=True, init_token=\"<s>\", eos_token=\"</s>\", lower=True, \n",
    "               include_lengths=True, batch_first=True)\n",
    "TARGET = Field(tokenize=str.split, use_vocab=True, init_token=\"<s>\", eos_token=\"</s>\", lower=True, \n",
    "               batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = \\\n",
    "    TabularDataset.splits(path='data/en_fa/', format='tsv', train=train_file, validation=valid_file, test=test_file,\n",
    "                          fields=[('so', SOURCE), ('ta', TARGET)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE.build_vocab(train_data)\n",
    "TARGET.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = BucketIterator(train_data, batch_size=BATCH_SIZE, device=DEVICE,\n",
    "                              sort_key=lambda x: len(x.so), sort_within_batch=True, repeat=False)\n",
    "train_loader = BucketIterator(valid_data, batch_size=BATCH_SIZE, device=DEVICE,\n",
    "                              sort_key=lambda x: len(x.so), sort_within_batch=True, repeat=False)\n",
    "train_loader = BucketIterator(test_data, batch_size=BATCH_SIZE, device=DEVICE,\n",
    "                              sort_key=lambda x: len(x.so), sort_within_batch=True, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5229, 9075)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SOURCE.vocab), len(TARGET.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/encoder_decoder_att.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, V_e, m_e, n_e, num_layers=1, bidrec=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"\"\"\n",
    "        vocab_size: V_e\n",
    "        embed_size: m_e\n",
    "        hidden_size: n_e\n",
    "        \"\"\"\n",
    "        self.V_e = V_e\n",
    "        self.m_e = m_e\n",
    "        self.n_e = n_e\n",
    "        self.num_layers = num_layers\n",
    "        self.bidrec = bidrec\n",
    "        self.n_direct = 2 if bidrec else 1\n",
    "        \n",
    "        self.embed = nn.Embedding(V_e, m_e) \n",
    "        self.gru = nn.GRU(m_e, n_e, num_layers, batch_first=True, bidirectional=bidrec)\n",
    "        \n",
    "    def forward(self, inputs, lengths):\n",
    "        \"\"\"\n",
    "        input: \n",
    "        - inputs: B, T_x\n",
    "        - lengths: actual max length of batches\n",
    "        output:\n",
    "        - outputs: B, T_x, n_e\n",
    "        \"\"\"\n",
    "        # embeded: (B, T_x, n_e)\n",
    "        embeded = self.embed(inputs) \n",
    "        # packed: (B*T_x, n_e)\n",
    "        packed = pack_padded_sequence(embeded, lengths, batch_first=True) \n",
    "        # packed outputs: (B*T_x, 2*n_e)\n",
    "        # hidden: (num of layers*n_direct, B, 2*n_e)\n",
    "        outputs, hidden = self.gru(packed)\n",
    "        # unpacked outputs: (B, T_x, 2*n_e)\n",
    "        outputs, output_lengths = pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        # hidden bidirection: (num of layers*n_direct(0,1,2...last one), B, n_e)\n",
    "        # choosen last hidden: (B, 1, 2*n_e)\n",
    "        hidden = torch.cat([h for h in hidden[-self.n_direct:]], 1).unsqueeze(1)\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, hidden_size2=None, method='general'):\n",
    "        super(Attention, self).__init__()\n",
    "        \"\"\"\n",
    "        hidden_size: set hidden size same as decoder hidden size which is n_d (= 2*n_e)\n",
    "        hidden_size2: only for concat&paper method, if none then is same as hidden_size (n_d)\n",
    "        (in paper notation is n', https://arxiv.org/abs/1409.0473)\n",
    "        methods:\n",
    "        - 'dot': dot product between hidden and encoder_outputs\n",
    "        - 'general': encoder_outputs through a linear layer \n",
    "        - 'concat': concat (hidden, encoder_outputs)\n",
    "        - 'paper': concat + tanh\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size \n",
    "        self.hidden_size2 = hidden_size2 if hidden_size2 else hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size) \n",
    "            # linear_weight shape: (out_f, in_f)\n",
    "            # linear input: (B, *, in_f)\n",
    "            # linear output: (B, *, out_f)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size*2, self.hidden_size2)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, self.hidden_size2))\n",
    "        \n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, encoder_lengths=None, return_weight=False):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        - hidden, previous hidden(= H): B, 1, n_d \n",
    "        - encoder_outputs, source context(= O): B, T_x, n_d\n",
    "        - encoder_lengths: real lengths of encoder outputs\n",
    "        - return_weight = return weights(alphas)\n",
    "        output:\n",
    "        - attentioned_hidden(= z): B, 1, n_d\n",
    "        - weights(= w): B, 1, T_x\n",
    "        \"\"\"\n",
    "        H, O = hidden, encoder_outputs\n",
    "        # Batch(B), Seq_length(T), dimemsion(n)\n",
    "        B_H, T_H, n_H = H.size()\n",
    "        B_O, T_O, n_O = O.size()\n",
    "        \n",
    "        if B_H != B_O:\n",
    "            msg = \"Batch size is not correct, H: {} O: {}\".format(H.size(), O.size())\n",
    "            raise ValueError(msg)\n",
    "        else:\n",
    "            B = B_H\n",
    "        \n",
    "        # score: (B, 1, T_x)\n",
    "        s = self.score(H, O) \n",
    "        \n",
    "        # encoding masking: ensure not to calculate paddings\n",
    "        if encoder_lengths is not None:\n",
    "            mask = s.data.new(B, T_H, T_O) # (B, 1, T_x)\n",
    "            mask = self.fill_context_mask(mask, sizes=encoder_lengths, v_mask=float('-inf'), v_unmask=0)\n",
    "            s += mask\n",
    "        \n",
    "        # softmax: (B, 1, T_x)\n",
    "        w = F.softmax(s, 2) \n",
    "        \n",
    "        # attention: weight * encoder_hiddens, (B, 1, T_x) * (B, T_x, n_d) = (B, 1, n_d)\n",
    "        z = w.bmm(O)\n",
    "        if return_weight:\n",
    "            return z, w\n",
    "        return z\n",
    "    \n",
    "    def score(self, H, O):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - hiddden, previous hidden(= H): B, 1, n_d \n",
    "        - encoder_outputs, source context(= O): B, T_x, n_d\n",
    "        \"\"\"\n",
    "        if self.method == 'dot':\n",
    "            # bmm: (B, 1, n_d) * (B, n_d, T_x) = (B, 1, T_x)\n",
    "            e = H.bmm(O.transpose(1, 2))\n",
    "            return e\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            # attn: (B, T_x, n_d) > (B, T_x, n_d)\n",
    "            # bmm: (B, 1, n_d) * (B, n_d, T_x) = (B, 1, T_x)\n",
    "            e = self.attn(O)\n",
    "            e = H.bmm(e.transpose(1, 2))\n",
    "            return e\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            # H repeat: (B, 1, n_d) > (B, T_x, n_d)\n",
    "            # cat: (B, T_x, 2*n_d)\n",
    "            # attn: (B, T_x, 2*n_d) > (B, T_x, n_d)\n",
    "            # v repeat: (1, n_d) > (B, 1, n_d)\n",
    "            # bmm: (B, 1, n_d) * (B, n_d, T_x) = (B, 1, T_x)\n",
    "            cat = torch.cat([H.repeat(1, O.size(1), 1), O], 2)\n",
    "            e = self.attn(cat)\n",
    "            v = self.v.repeat(O.size(0), 1).unsqueeze(1)\n",
    "            e = v.bmm(e.transpose(1, 2))\n",
    "            return e\n",
    "        \n",
    "        elif self.method == 'paper':\n",
    "            # add tanh after attention linear layer in 'concat' method\n",
    "            cat = torch.cat([H.repeat(1, O.size(1), 1), O], 2)\n",
    "            e = F.tanh(self.attn(cat))\n",
    "            v = self.v.repeat(O.size(0), 1).unsqueeze(1)\n",
    "            e = v.bmm(e.transpose(1, 2))\n",
    "            return e\n",
    "    \n",
    "    def fill_context_mask(self, mask, sizes, v_mask, v_unmask):\n",
    "        \"\"\"Fill attention mask inplace for a variable length context.\n",
    "        Args\n",
    "        ----\n",
    "        mask: Tensor of size (B, T, D)\n",
    "            Tensor to fill with mask values. \n",
    "        sizes: list[int]\n",
    "            List giving the size of the context for each item in\n",
    "            the batch. Positions beyond each size will be masked.\n",
    "        v_mask: float\n",
    "            Value to use for masked positions.\n",
    "        v_unmask: float\n",
    "            Value to use for unmasked positions.\n",
    "        Returns\n",
    "        -------\n",
    "        mask:\n",
    "            Filled with values in {v_mask, v_unmask}\n",
    "        \"\"\"\n",
    "        mask.fill_(v_unmask)\n",
    "        n_context = mask.size(2)\n",
    "        for i, size in enumerate(sizes):\n",
    "            if size < n_context:\n",
    "                mask[i,:,size:] = v_mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, V_d, m_d, n_d, sos_idx, num_layers=1, hidden_size2=None, method='general', \n",
    "                 return_weight=True, max_len=15):\n",
    "        super(Decoder, self).__init__()\n",
    "        \"\"\"\n",
    "        vocab_size: V_d\n",
    "        embed_size: m_d\n",
    "        hidden_size: n_d (set this value as 2*n_e)\n",
    "        methods:\n",
    "        - 'dot': dot product between hidden and encoder_outputs\n",
    "        - 'general': encoder_outputs through a linear layer \n",
    "        - 'concat': concat (hidden, encoder_outputs)\n",
    "        - 'paper': concat + tanh\n",
    "        return_weight: return attention weights\n",
    "        \"\"\"\n",
    "        self.V_d = V_d\n",
    "        self.m_d = m_d\n",
    "        self.n_d = n_d\n",
    "        self.sos_idx = sos_idx\n",
    "        self.num_layers = num_layers\n",
    "        self.return_weight = return_weight\n",
    "        self.method = method\n",
    "        # attention\n",
    "        self.attention = Attention(hidden_size=n_d, hidden_size2=hidden_size2, method=method)\n",
    "        # embed\n",
    "        self.embed = nn.Embedding(V_d, m_d)\n",
    "        # gru(W*[embed, context] + U*[hidden_prev])\n",
    "        # gru: m+n\n",
    "        self.gru = nn.GRU(m_d+n_d, n_d, num_layers, batch_first=True, bidirectional=False) \n",
    "        # linear\n",
    "        self.linear = nn.Linear(2*n_d, V_d)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        \n",
    "    def start_token(self, batch_size):\n",
    "        sos = torch.LongTensor([self.sos_idx]*batch_size).unsqueeze(1)\n",
    "        if USE_CUDA: sos = sos.cuda()\n",
    "        return sos\n",
    "    \n",
    "    def forward(self, hidden, enc_outputs, enc_outputs_lengths=None, max_len=None):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        - hidden(previous hidden): B, 1, n_d \n",
    "        - enc_outputs(source context): B, T_x, n_d\n",
    "        - enc_outputs_lengths: list type\n",
    "        - max_len(targer sentences max len in batch): T_y\n",
    "        \"\"\"\n",
    "        if max_len is None: max_len = self.max_len\n",
    "        \n",
    "        inputs = self.start_token(hidden.size(0)) # (B, 1)\n",
    "        embeded = self.embed(inputs) # (B, 1, m_d)\n",
    "        # prepare for whole targer sentence scores\n",
    "        scores = []\n",
    "        attn_weights = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            # context vector: previous hidden(s{i-1}), encoder_outputs(O_e) > context(c{i}), weights\n",
    "            # - context: (B, 1, n_d)\n",
    "            # - weights: (B, 1, T_x)\n",
    "            context, weights = self.attention(hidden, enc_outputs, enc_outputs_lengths, \n",
    "                                              return_weight=self.return_weight)\n",
    "            attn_weights.append(weights.squeeze(1))\n",
    "            \n",
    "            # concat context & embedding vectors: (B, 1, m_d+n_d)\n",
    "            gru_input = torch.cat([embeded, context], 2)\n",
    "            \n",
    "            # gru((context&embedding), previous hidden)\n",
    "            # output hidden(s{i}): (1, B, n_d)\n",
    "            _, hidden = self.gru(gru_input, hidden.transpose(0, 1))\n",
    "            hidden = hidden.transpose(0, 1) # change shape to (B, 1, n_d) again\n",
    "            \n",
    "            # concat context and new hidden vectors: (B, 1, 2*n_d)\n",
    "            concated = torch.cat([hidden, context], 2)\n",
    "            \n",
    "            # get score: (B, V_d)\n",
    "            score = self.linear(concated.squeeze(1))\n",
    "            scores.append(score)\n",
    "            \n",
    "            # greedy method\n",
    "            decoded = score.max(1)[1]  # (B)\n",
    "            embeded = self.embed(decoded).unsqueeze(1) # next input y{i-1} (B, 1, m_d) \n",
    "\n",
    "        # column-wise concat, reshape!! \n",
    "        # scores = [(B, V_d), (B, V_d), (B, V_d)...] > (B, V_d*max_len)\n",
    "        # attn_weights = [(B, T_x), (B, T_x), (B, T_x)...] > (B*max_len, T_x)\n",
    "        scores = torch.cat(scores, 1)\n",
    "        return scores.view(inputs.size(0)*max_len, -1), torch.cat(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_so = len(SOURCE.vocab)\n",
    "V_ta = len(TARGET.vocab)\n",
    "HIDDEN = 500\n",
    "EMBED = 100\n",
    "STEP = 200\n",
    "LR = 0.001\n",
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(V_so, EMBED, HIDDEN, NUM_LAYERS, bidrec=True)\n",
    "dec = Decoder(V_ta, EMBED, 2*HIDDEN, sos_idx=SOURCE.vocab.stoi['<s>'], method='general')\n",
    "if USE_CUDA:\n",
    "    enc = enc.cuda()\n",
    "    dec = dec.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=TARGET.vocab.stoi['<pad>'])\n",
    "optimizer = optim.Adam(list(enc.parameters()) + list(dec.parameters()), lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.1, milestones=[100], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200] train_loss: 3.0524, lr: 0.001\n",
      "[11/200] train_loss: 0.6400, lr: 0.001\n",
      "[21/200] train_loss: 0.5111, lr: 0.001\n",
      "[31/200] train_loss: 0.4594, lr: 0.001\n",
      "[41/200] train_loss: 0.4230, lr: 0.001\n",
      "[51/200] train_loss: 0.4023, lr: 0.001\n",
      "[61/200] train_loss: 0.3802, lr: 0.001\n",
      "[71/200] train_loss: 0.3608, lr: 0.001\n",
      "[81/200] train_loss: 0.3556, lr: 0.001\n",
      "[91/200] train_loss: 0.3464, lr: 0.001\n",
      "[101/200] train_loss: 0.2895, lr: 0.0001\n",
      "[111/200] train_loss: 0.2172, lr: 0.0001\n",
      "[121/200] train_loss: 0.2101, lr: 0.0001\n",
      "[131/200] train_loss: 0.2085, lr: 0.0001\n",
      "[141/200] train_loss: 0.2064, lr: 0.0001\n",
      "[151/200] train_loss: 0.2059, lr: 0.0001\n",
      "[161/200] train_loss: 0.2048, lr: 0.0001\n",
      "[171/200] train_loss: 0.2046, lr: 0.0001\n",
      "[181/200] train_loss: 0.2042, lr: 0.0001\n",
      "[191/200] train_loss: 0.2029, lr: 0.0001\n"
     ]
    }
   ],
   "source": [
    "enc.train()\n",
    "dec.train()\n",
    "for step in range(STEP):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, lengths = batch.so\n",
    "        targets = batch.ta\n",
    "        \n",
    "        enc.zero_grad()\n",
    "        dec.zero_grad()\n",
    "        \n",
    "        output, hidden = enc(inputs, lengths.tolist())\n",
    "        preds, _ = dec(hidden, output, lengths.tolist(), targets.size(1)) # max_len\n",
    "        \n",
    "        loss = loss_function(preds, targets.view(-1))\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        msg = '[{}/{}] train_loss: {:.4f}, lr: {}'.format(\\\n",
    "          step+1, STEP, np.mean(losses), round(scheduler.get_lr()[0], 6))\n",
    "        print(msg)\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc.state_dict(), './data/model/fra_eng.enc')\n",
    "torch.save(dec.state_dict(), './data/model/fra_eng.dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(V_so, EMBED, HIDDEN, NUM_LAYERS, bidrec=True)\n",
    "dec = Decoder(V_ta, EMBED, 2*HIDDEN, sos_idx=SOURCE.vocab.stoi['<s>'], method='general')\n",
    "if USE_CUDA:\n",
    "    enc = enc.cuda()\n",
    "    dec = dec.cuda()\n",
    "    \n",
    "enc.load_state_dict(torch.load('./data/model/fra_eng.enc'))\n",
    "dec.load_state_dict(torch.load('./data/model/fra_eng.dec'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test & Visualize Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed code from https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "\n",
    "def show_attention(input_words, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_words, rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentence(x):\n",
    "    return [i for i in x if i not in ['<s>', '</s>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source :  everything is normal .\n",
      "Truth :  tout est normal .\n",
      "Prediction :  tout est normal .\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc.eval()\n",
    "dec.eval()\n",
    "\n",
    "test = random.choice(train_data)\n",
    "source_sentence = test.so\n",
    "target_sentence = test.ta\n",
    "\n",
    "s = torch.LongTensor(list(map(lambda x: SOURCE.vocab.stoi[x], source_sentence))).view(1, -1)\n",
    "t = torch.LongTensor(list(map(lambda x: TARGET.vocab.stoi[x], target_sentence))).view(1, -1)\n",
    "if USE_CUDA:\n",
    "    s = s.cuda()\n",
    "    t = t.cuda()\n",
    "\n",
    "output, hidden = enc(s, [s.size(1)])\n",
    "pred, attn = dec(hidden, output)\n",
    "pred_sentence = [TARGET.vocab.itos[i] for i in pred.max(1)[1]]\n",
    "\n",
    "\n",
    "print('Source : ', ' '.join(print_sentence(source_sentence)))\n",
    "print('Truth : ', ' '.join(print_sentence(target_sentence)))\n",
    "print('Prediction : ', ' '.join(print_sentence(pred_sentence)))\n",
    "\n",
    "if USE_CUDA:\n",
    "    attn = attn.cpu()\n",
    "\n",
    "show_attention(source_sentence, pred_sentence, attn.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
