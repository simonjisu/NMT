{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"NMT by Jointly Learning to Align and Translate\" Implementation\n",
    "\n",
    "original paper: https://arxiv.org/abs/1409.0473"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references\n",
    "* arichitecture picture: https://arxiv.org/pdf/1703.03906.pdf\n",
    "* tutorial: https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "* data source: http://www.statmt.org/wmt14/translation-task.html\n",
    "* data source2: http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field, Iterator, BucketIterator, TabularDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "# others\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.cuda.current_device()\n",
    "# USE_CUDA = False\n",
    "# DEVICE = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = 'data/en_de/en_de_1000.txt' # cannot use because of gpu memory is not enough\n",
    "train_file = 'eng-fra-filtered.train'\n",
    "valid_file = 'eng-fra-filtered.valid'\n",
    "test_file = 'eng-fra-filtered.test'\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = Field(tokenize=str.split, use_vocab=True, init_token=\"<s>\", eos_token=\"</s>\", lower=True, \n",
    "               include_lengths=True, batch_first=True)\n",
    "TARGET = Field(tokenize=str.split, use_vocab=True, init_token=\"<s>\", eos_token=\"</s>\", lower=True, \n",
    "               batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = \\\n",
    "    TabularDataset.splits(path='data/en_fa/', format='tsv', train=train_file, validation=valid_file, test=test_file,\n",
    "                          fields=[('so', SOURCE), ('ta', TARGET)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE.build_vocab(train_data)\n",
    "TARGET.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = BucketIterator(train_data, batch_size=BATCH_SIZE, device=DEVICE,\n",
    "                              sort_key=lambda x: len(x.so), sort_within_batch=True, repeat=False)\n",
    "valid_loader = BucketIterator(valid_data, batch_size=BATCH_SIZE, device=DEVICE,\n",
    "                              sort_key=lambda x: len(x.so), sort_within_batch=True, repeat=False)\n",
    "test_loader = BucketIterator(test_data, batch_size=BATCH_SIZE, device=DEVICE,\n",
    "                              sort_key=lambda x: len(x.so), sort_within_batch=True, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10322, 17526)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SOURCE.vocab), len(TARGET.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'studied', 'for', 'a', 'while', 'in', 'the', 'afternoon', '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[-1].so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/encoder_decoder_att.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, V_e, m_e, n_e, num_layers=1, bidrec=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"\"\"\n",
    "        vocab_size: V_e\n",
    "        embed_size: m_e\n",
    "        hidden_size: n_e\n",
    "        \"\"\"\n",
    "        self.V_e = V_e\n",
    "        self.m_e = m_e\n",
    "        self.n_e = n_e\n",
    "        self.num_layers = num_layers\n",
    "        self.bidrec = bidrec\n",
    "        self.n_direct = 2 if bidrec else 1\n",
    "        \n",
    "        self.embed = nn.Embedding(V_e, m_e) \n",
    "        self.gru = nn.GRU(m_e, n_e, num_layers, batch_first=True, bidirectional=bidrec)\n",
    "        \n",
    "    def forward(self, inputs, lengths):\n",
    "        \"\"\"\n",
    "        input: \n",
    "        - inputs: B, T_x\n",
    "        - lengths: actual max length of batches\n",
    "        output:\n",
    "        - outputs: B, T_x, n_e\n",
    "        \"\"\"\n",
    "        # embeded: (B, T_x, n_e)\n",
    "        embeded = self.embed(inputs) \n",
    "        # packed: (B*T_x, n_e)\n",
    "        packed = pack_padded_sequence(embeded, lengths, batch_first=True) \n",
    "        # packed outputs: (B*T_x, 2*n_e)\n",
    "        # hidden: (num of layers*n_direct, B, 2*n_e)\n",
    "        outputs, hidden = self.gru(packed)\n",
    "        # unpacked outputs: (B, T_x, 2*n_e)\n",
    "        outputs, output_lengths = pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        # hidden bidirection: (num of layers*n_direct(0,1,2...last one), B, n_e)\n",
    "        # choosen last hidden: (B, 1, 2*n_e)\n",
    "        hidden = torch.cat([h for h in hidden[-self.n_direct:]], 1).unsqueeze(1)\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, hidden_size2=None, method='general'):\n",
    "        super(Attention, self).__init__()\n",
    "        \"\"\"\n",
    "        hidden_size: set hidden size same as decoder hidden size which is n_d (= 2*n_e)\n",
    "        hidden_size2: only for concat&paper method, if none then is same as hidden_size (n_d)\n",
    "        (in paper notation is n', https://arxiv.org/abs/1409.0473)\n",
    "        methods:\n",
    "        - 'dot': dot product between hidden and encoder_outputs\n",
    "        - 'general': encoder_outputs through a linear layer \n",
    "        - 'concat': concat (hidden, encoder_outputs)\n",
    "        - 'paper': concat + tanh\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size \n",
    "        self.hidden_size2 = hidden_size2 if hidden_size2 else hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size) \n",
    "            # linear_weight shape: (out_f, in_f)\n",
    "            # linear input: (B, *, in_f)\n",
    "            # linear output: (B, *, out_f)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size*2, self.hidden_size2)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, self.hidden_size2))\n",
    "        \n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, encoder_lengths=None, return_weight=False):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        - hidden, previous hidden(= H): B, 1, n_d \n",
    "        - encoder_outputs, source context(= O): B, T_x, n_d\n",
    "        - encoder_lengths: real lengths of encoder outputs\n",
    "        - return_weight = return weights(alphas)\n",
    "        output:\n",
    "        - attentioned_hidden(= z): B, 1, n_d\n",
    "        - weights(= w): B, 1, T_x\n",
    "        \"\"\"\n",
    "        H, O = hidden, encoder_outputs\n",
    "        # Batch(B), Seq_length(T), dimemsion(n)\n",
    "        B_H, T_H, n_H = H.size()\n",
    "        B_O, T_O, n_O = O.size()\n",
    "        \n",
    "        if B_H != B_O:\n",
    "            msg = \"Batch size is not correct, H: {} O: {}\".format(H.size(), O.size())\n",
    "            raise ValueError(msg)\n",
    "        else:\n",
    "            B = B_H\n",
    "        \n",
    "        # score: (B, 1, T_x)\n",
    "        s = self.score(H, O) \n",
    "        \n",
    "        # encoding masking: ensure not to calculate paddings\n",
    "        if encoder_lengths is not None:\n",
    "            mask = s.data.new(B, T_H, T_O) # (B, 1, T_x)\n",
    "            mask = self.fill_context_mask(mask, sizes=encoder_lengths, v_mask=float('-inf'), v_unmask=0)\n",
    "            s += mask\n",
    "        \n",
    "        # softmax: (B, 1, T_x)\n",
    "        w = F.softmax(s, 2) \n",
    "        \n",
    "        # attention: weight * encoder_hiddens, (B, 1, T_x) * (B, T_x, n_d) = (B, 1, n_d)\n",
    "        z = w.bmm(O)\n",
    "        if return_weight:\n",
    "            return z, w\n",
    "        return z\n",
    "    \n",
    "    def score(self, H, O):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - hiddden, previous hidden(= H): B, 1, n_d \n",
    "        - encoder_outputs, source context(= O): B, T_x, n_d\n",
    "        \"\"\"\n",
    "        if self.method == 'dot':\n",
    "            # bmm: (B, 1, n_d) * (B, n_d, T_x) = (B, 1, T_x)\n",
    "            e = H.bmm(O.transpose(1, 2))\n",
    "            return e\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            # attn: (B, T_x, n_d) > (B, T_x, n_d)\n",
    "            # bmm: (B, 1, n_d) * (B, n_d, T_x) = (B, 1, T_x)\n",
    "            e = self.attn(O)\n",
    "            e = H.bmm(e.transpose(1, 2))\n",
    "            return e\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            # H repeat: (B, 1, n_d) > (B, T_x, n_d)\n",
    "            # cat: (B, T_x, 2*n_d)\n",
    "            # attn: (B, T_x, 2*n_d) > (B, T_x, n_d)\n",
    "            # v repeat: (1, n_d) > (B, 1, n_d)\n",
    "            # bmm: (B, 1, n_d) * (B, n_d, T_x) = (B, 1, T_x)\n",
    "            cat = torch.cat([H.repeat(1, O.size(1), 1), O], 2)\n",
    "            e = self.attn(cat)\n",
    "            v = self.v.repeat(O.size(0), 1).unsqueeze(1)\n",
    "            e = v.bmm(e.transpose(1, 2))\n",
    "            return e\n",
    "        \n",
    "        elif self.method == 'paper':\n",
    "            # add tanh after attention linear layer in 'concat' method\n",
    "            cat = torch.cat([H.repeat(1, O.size(1), 1), O], 2)\n",
    "            e = F.tanh(self.attn(cat))\n",
    "            v = self.v.repeat(O.size(0), 1).unsqueeze(1)\n",
    "            e = v.bmm(e.transpose(1, 2))\n",
    "            return e\n",
    "    \n",
    "    def fill_context_mask(self, mask, sizes, v_mask, v_unmask):\n",
    "        \"\"\"Fill attention mask inplace for a variable length context.\n",
    "        Args\n",
    "        ----\n",
    "        mask: Tensor of size (B, T, D)\n",
    "            Tensor to fill with mask values. \n",
    "        sizes: list[int]\n",
    "            List giving the size of the context for each item in\n",
    "            the batch. Positions beyond each size will be masked.\n",
    "        v_mask: float\n",
    "            Value to use for masked positions.\n",
    "        v_unmask: float\n",
    "            Value to use for unmasked positions.\n",
    "        Returns\n",
    "        -------\n",
    "        mask:\n",
    "            Filled with values in {v_mask, v_unmask}\n",
    "        \"\"\"\n",
    "        mask.fill_(v_unmask)\n",
    "        n_context = mask.size(2)\n",
    "        for i, size in enumerate(sizes):\n",
    "            if size < n_context:\n",
    "                mask[i,:,size:] = v_mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, V_d, m_d, n_d, sos_idx, num_layers=1, hidden_size2=None, method='general', \n",
    "                 return_weight=True, max_len=15):\n",
    "        super(Decoder, self).__init__()\n",
    "        \"\"\"\n",
    "        vocab_size: V_d\n",
    "        embed_size: m_d\n",
    "        hidden_size: n_d (set this value as 2*n_e)\n",
    "        methods:\n",
    "        - 'dot': dot product between hidden and encoder_outputs\n",
    "        - 'general': encoder_outputs through a linear layer \n",
    "        - 'concat': concat (hidden, encoder_outputs)\n",
    "        - 'paper': concat + tanh\n",
    "        return_weight: return attention weights\n",
    "        \"\"\"\n",
    "        self.V_d = V_d\n",
    "        self.m_d = m_d\n",
    "        self.n_d = n_d\n",
    "        self.sos_idx = sos_idx\n",
    "        self.num_layers = num_layers\n",
    "        self.return_weight = return_weight\n",
    "        self.method = method\n",
    "        # attention\n",
    "        self.attention = Attention(hidden_size=n_d, hidden_size2=hidden_size2, method=method)\n",
    "        # embed\n",
    "        self.embed = nn.Embedding(V_d, m_d)\n",
    "        # gru(W*[embed, context] + U*[hidden_prev])\n",
    "        # gru: m+n\n",
    "        self.gru = nn.GRU(m_d+n_d, n_d, num_layers, batch_first=True, bidirectional=False) \n",
    "        # linear\n",
    "        self.linear = nn.Linear(2*n_d, V_d)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        \n",
    "    def start_token(self, batch_size):\n",
    "        sos = torch.LongTensor([self.sos_idx]*batch_size).unsqueeze(1)\n",
    "        if USE_CUDA: sos = sos.cuda()\n",
    "        return sos\n",
    "    \n",
    "    def forward(self, hidden, enc_outputs, enc_outputs_lengths=None, max_len=None):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        - hidden(previous hidden): B, 1, n_d \n",
    "        - enc_outputs(source context): B, T_x, n_d\n",
    "        - enc_outputs_lengths: list type\n",
    "        - max_len(targer sentences max len in batch): T_y\n",
    "        \"\"\"\n",
    "        if max_len is None: max_len = self.max_len\n",
    "        \n",
    "        inputs = self.start_token(hidden.size(0)) # (B, 1)\n",
    "        embeded = self.embed(inputs) # (B, 1, m_d)\n",
    "        # prepare for whole targer sentence scores\n",
    "        scores = []\n",
    "        attn_weights = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            # context vector: previous hidden(s{i-1}), encoder_outputs(O_e) > context(c{i}), weights\n",
    "            # - context: (B, 1, n_d)\n",
    "            # - weights: (B, 1, T_x)\n",
    "            context, weights = self.attention(hidden, enc_outputs, enc_outputs_lengths, \n",
    "                                              return_weight=self.return_weight)\n",
    "            attn_weights.append(weights.squeeze(1))\n",
    "            \n",
    "            # concat context & embedding vectors: (B, 1, m_d+n_d)\n",
    "            gru_input = torch.cat([embeded, context], 2)\n",
    "            \n",
    "            # gru((context&embedding), previous hidden)\n",
    "            # output hidden(s{i}): (1, B, n_d)\n",
    "            _, hidden = self.gru(gru_input, hidden.transpose(0, 1))\n",
    "            hidden = hidden.transpose(0, 1) # change shape to (B, 1, n_d) again\n",
    "            \n",
    "            # concat context and new hidden vectors: (B, 1, 2*n_d)\n",
    "            concated = torch.cat([hidden, context], 2)\n",
    "            \n",
    "            # get score: (B, V_d)\n",
    "            score = self.linear(concated.squeeze(1))\n",
    "            scores.append(score)\n",
    "            \n",
    "            # greedy method\n",
    "            decoded = score.max(1)[1]  # (B)\n",
    "            embeded = self.embed(decoded).unsqueeze(1) # next input y{i-1} (B, 1, m_d) \n",
    "\n",
    "        # column-wise concat, reshape!! \n",
    "        # scores = [(B, V_d), (B, V_d), (B, V_d)...] > (B, V_d*max_len)\n",
    "        # attn_weights = [(B, T_x), (B, T_x), (B, T_x)...] > (B*max_len, T_x)\n",
    "        scores = torch.cat(scores, 1)\n",
    "        return scores.view(inputs.size(0)*max_len, -1), torch.cat(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_so = len(SOURCE.vocab)\n",
    "V_ta = len(TARGET.vocab)\n",
    "HIDDEN = 600\n",
    "EMBED = 300\n",
    "STEP = 200\n",
    "LR = 0.001\n",
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = BATCH_SIZE\n",
    "EARLY_STOPPING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(V_so, EMBED, HIDDEN, NUM_LAYERS, bidrec=True)\n",
    "dec = Decoder(V_ta, EMBED, 2*HIDDEN, sos_idx=SOURCE.vocab.stoi['<s>'], method='general')\n",
    "if USE_CUDA:\n",
    "    enc = enc.cuda()\n",
    "    dec = dec.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=TARGET.vocab.stoi['<pad>'])\n",
    "optimizer = optim.Adam(list(enc.parameters()) + list(dec.parameters()), lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.1, milestones=[100], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200] train_loss: 3.0524, lr: 0.001\n",
      "[11/200] train_loss: 0.6400, lr: 0.001\n",
      "[21/200] train_loss: 0.5111, lr: 0.001\n",
      "[31/200] train_loss: 0.4594, lr: 0.001\n",
      "[41/200] train_loss: 0.4230, lr: 0.001\n",
      "[51/200] train_loss: 0.4023, lr: 0.001\n",
      "[61/200] train_loss: 0.3802, lr: 0.001\n",
      "[71/200] train_loss: 0.3608, lr: 0.001\n",
      "[81/200] train_loss: 0.3556, lr: 0.001\n",
      "[91/200] train_loss: 0.3464, lr: 0.001\n",
      "[101/200] train_loss: 0.2895, lr: 0.0001\n",
      "[111/200] train_loss: 0.2172, lr: 0.0001\n",
      "[121/200] train_loss: 0.2101, lr: 0.0001\n",
      "[131/200] train_loss: 0.2085, lr: 0.0001\n",
      "[141/200] train_loss: 0.2064, lr: 0.0001\n",
      "[151/200] train_loss: 0.2059, lr: 0.0001\n",
      "[161/200] train_loss: 0.2048, lr: 0.0001\n",
      "[171/200] train_loss: 0.2046, lr: 0.0001\n",
      "[181/200] train_loss: 0.2042, lr: 0.0001\n",
      "[191/200] train_loss: 0.2029, lr: 0.0001\n"
     ]
    }
   ],
   "source": [
    "enc.train()\n",
    "dec.train()\n",
    "for step in range(STEP):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, lengths = batch.so\n",
    "        targets = batch.ta\n",
    "        \n",
    "        enc.zero_grad()\n",
    "        dec.zero_grad()\n",
    "        \n",
    "        output, hidden = enc(inputs, lengths.tolist())\n",
    "        preds, _ = dec(hidden, output, lengths.tolist(), targets.size(1)) # max_len\n",
    "        \n",
    "        loss = loss_function(preds, targets.view(-1))\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        msg = '[{}/{}] train_loss: {:.4f}, lr: {}'.format(\\\n",
    "          step+1, STEP, np.mean(losses), round(scheduler.get_lr()[0], 6))\n",
    "        print(msg)\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc.state_dict(), './data/model/fra_eng.enc')\n",
    "torch.save(dec.state_dict(), './data/model/fra_eng.dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(V_so, EMBED, HIDDEN, NUM_LAYERS, bidrec=True)\n",
    "dec = Decoder(V_ta, EMBED, 2*HIDDEN, sos_idx=SOURCE.vocab.stoi['<s>'], method='general')\n",
    "if USE_CUDA:\n",
    "    enc = enc.cuda()\n",
    "    dec = dec.cuda()\n",
    "    \n",
    "enc.load_state_dict(torch.load('./data/model/fra_eng.enc'))\n",
    "dec.load_state_dict(torch.load('./data/model/fra_eng.dec'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test & Visualize Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed code from https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "\n",
    "def show_attention(input_words, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_words, rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentence(x):\n",
    "    return [i for i in x if i not in ['<s>', '</s>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source :  something was going on .\n",
      "Truth :  il se passait quelque chose .\n",
      "Prediction :  il est a pas d chose . .\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc.eval()\n",
    "dec.eval()\n",
    "\n",
    "test = random.choice(train_data)\n",
    "source_sentence = test.so\n",
    "target_sentence = test.ta\n",
    "\n",
    "s = torch.LongTensor(list(map(lambda x: SOURCE.vocab.stoi[x], source_sentence))).view(1, -1)\n",
    "t = torch.LongTensor(list(map(lambda x: TARGET.vocab.stoi[x], target_sentence))).view(1, -1)\n",
    "if USE_CUDA:\n",
    "    s = s.cuda()\n",
    "    t = t.cuda()\n",
    "\n",
    "output, hidden = enc(s, [s.size(1)])\n",
    "pred, attn = dec(hidden, output)\n",
    "pred_sentence = [TARGET.vocab.itos[i] for i in pred.max(1)[1]]\n",
    "\n",
    "\n",
    "print('Source : ', ' '.join(print_sentence(source_sentence)))\n",
    "print('Truth : ', ' '.join(print_sentence(target_sentence)))\n",
    "print('Prediction : ', ' '.join(print_sentence(pred_sentence)))\n",
    "\n",
    "if USE_CUDA:\n",
    "    attn = attn.cpu()\n",
    "\n",
    "show_attention(source_sentence, pred_sentence, attn.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
