Namespace(BATCH=20, DATATYPE='iwslt', DECLR=2.0, DEC_N_LAYER=3, DROP_RATE=0.2, EMBED=300, EMPTY_CUDA_MEMORY=True, ENC_N_LAYER=3, HIDDEN=600, LAMBDA=1e-05, LOAD_DEC_PATH='./saved_models/iswlt_big.dec', LOAD_ENC_PATH='./saved_models/iswlt_big.enc', LOAD_MODEL=True, LR=0.0001, L_NORM=True, MAX_LEN=50, METHOD='general', MIN_FREQ=2, NO_VALID=False, OPTIM='adam', PRINT_EVERY=5, RETURN_W=False, ROOTPATH='../data/', SAVE_BEST=True, SAVE_DEC_PATH='./saved_models/iswlt_big2.dec', SAVE_ENC_PATH='./saved_models/iswlt_big2.enc', SAVE_MODEL=True, STEP=15, TF=True, THRES=5, USE_CUDA=True)
Source Language: 51823 words, Target Language: 30980 words
Training Examples: 189416, Validation Examples: 946
Building Model ...
Load complete!
----------------------------------------
 > [0/9471] train_loss 1.3671
Traceback (most recent call last):
  File "main.py", line 68, in <module>
    train_model(config, enc, dec, loss_function, enc_optimizer, dec_optimizer, enc_scheduler, dec_scheduler, train_loader, valid_loader)
  File "/home/simonjisu/code/NMT/model/train.py", line 190, in train_model
    loss_function, enc_optimizer, dec_optimizer)
  File "/home/simonjisu/code/NMT/model/train.py", line 148, in run_step
    loss.backward()
  File "/usr/local/lib/python3.6/site-packages/torch/tensor.py", line 93, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/usr/local/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: out of memory
