Namespace(BATCH=64, DECLR=5.0, DROPOUT=True, EARLY=False, EARLY_PATIENCE=5, EMBED=300, EVAL_EVERY=1, HIDDEN=600, HIDDEN2=None, LAMBDA=0.0001, LR=0.001, METHOD='general', MIN_DELTA=0.0, NUM_HIDDEN=3, PATH='./data/en_fa/', SAVE_DEC_PATH='./data/model/fra-eng1.dec', SAVE_ENC_PATH='./data/model/fra-eng1.enc', STEP=50, TEST_FILE='fra-eng-small.test', TRAIN_FILE='fra-eng-small.train', VALID_FILE='fra-eng-small.valid')
cuda states: True
[1/50] train_loss: 2.7443, valid_loss: 2.9505
[2/50] train_loss: 2.0207, valid_loss: 2.5943
[3/50] train_loss: 1.7790, valid_loss: 2.4872
[4/50] train_loss: 1.6373, valid_loss: 2.2797
[5/50] train_loss: 1.5112, valid_loss: 2.2037
[6/50] train_loss: 1.4107, valid_loss: 2.2719
[7/50] train_loss: 1.3094, valid_loss: 2.3024
[8/50] train_loss: 1.2273, valid_loss: 2.1249
[9/50] train_loss: 1.1777, valid_loss: 2.1626
[10/50] train_loss: 1.1434, valid_loss: 2.1698
[11/50] train_loss: 1.1106, valid_loss: 2.2093
[12/50] train_loss: 1.0899, valid_loss: 2.1642
[13/50] train_loss: 1.0866, valid_loss: 2.0517
[14/50] train_loss: 1.0671, valid_loss: 2.2658
[15/50] train_loss: 1.0590, valid_loss: 2.2020
[16/50] train_loss: 1.0758, valid_loss: 2.0831
[17/50] train_loss: 1.0027, valid_loss: 2.0437
[18/50] train_loss: 1.0136, valid_loss: 2.0989
[19/50] train_loss: 1.0128, valid_loss: 2.1622
[20/50] train_loss: 0.9959, valid_loss: 2.1156
[21/50] train_loss: 0.9929, valid_loss: 2.0434
[22/50] train_loss: 1.0016, valid_loss: 2.0268
[23/50] train_loss: 1.0035, valid_loss: 2.2427
[24/50] train_loss: 0.9779, valid_loss: 2.1930
[25/50] train_loss: 0.9679, valid_loss: 2.1388
[26/50] train_loss: 0.6799, valid_loss: 1.8921
[27/50] train_loss: 0.5694, valid_loss: 1.8644
[28/50] train_loss: 0.5271, valid_loss: 1.8650
[29/50] train_loss: 0.4987, valid_loss: 1.9149
[30/50] train_loss: 0.4762, valid_loss: 1.8562
[31/50] train_loss: 0.4576, valid_loss: 1.9033
[32/50] train_loss: 0.4416, valid_loss: 2.0043
[33/50] train_loss: 0.4277, valid_loss: 2.0148
[34/50] train_loss: 0.4153, valid_loss: 1.9383
[35/50] train_loss: 0.4040, valid_loss: 1.9251
[36/50] train_loss: 0.3925, valid_loss: 1.9437
[37/50] train_loss: 0.3847, valid_loss: 1.9060
[38/50] train_loss: 0.2967, valid_loss: 1.9602
[39/50] train_loss: 0.2784, valid_loss: 1.9839
[40/50] train_loss: 0.2711, valid_loss: 1.9503
[41/50] train_loss: 0.2662, valid_loss: 1.9775
[42/50] train_loss: 0.2626, valid_loss: 1.9972
[43/50] train_loss: 0.2595, valid_loss: 1.9861
[44/50] train_loss: 0.2568, valid_loss: 1.9876
[45/50] train_loss: 0.2532, valid_loss: 1.9785
[46/50] train_loss: 0.2499, valid_loss: 1.9933
[47/50] train_loss: 0.2469, valid_loss: 1.9981
[48/50] train_loss: 0.2440, valid_loss: 2.0284
[49/50] train_loss: 0.2423, valid_loss: 1.9906
[50/50] train_loss: 0.2390, valid_loss: 1.9617
