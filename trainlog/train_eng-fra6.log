Namespace(BATCH=64, DECLR=5.0, DROPOUT=True, EARLY=False, EARLY_PATIENCE=5, EMBED=300, EVAL_EVERY=1, HIDDEN=600, HIDDEN2=None, LAMBDA=0.0001, LR=0.001, METHOD='general', MIN_DELTA=0.0, NUM_HIDDEN=3, PATH='./data/en_fa/', SAVE_DEC_PATH='./data/model/fra_eng6.dec', SAVE_ENC_PATH='./data/model/fra_eng6.enc', STEP=50, TEST_FILE='eng-fra-small.test', TRAIN_FILE='eng-fra-small.train', VALID_FILE='eng-fra-small.valid')
cuda states: True
[1/50] train_loss: 3.2661, valid_loss: 3.1813
[2/50] train_loss: 2.5771, valid_loss: 2.9299
[3/50] train_loss: 2.3795, valid_loss: 2.8083
[4/50] train_loss: 2.2180, valid_loss: 2.7226
[5/50] train_loss: 2.1057, valid_loss: 2.6260
[6/50] train_loss: 1.9927, valid_loss: 2.5705
[7/50] train_loss: 1.9337, valid_loss: 2.5398
[8/50] train_loss: 1.8721, valid_loss: 2.5817
[9/50] train_loss: 1.8369, valid_loss: 2.4395
[10/50] train_loss: 1.7853, valid_loss: 2.5338
[11/50] train_loss: 1.7633, valid_loss: 2.4489
[12/50] train_loss: 1.7272, valid_loss: 2.4544
[13/50] train_loss: 1.7311, valid_loss: 2.4177
[14/50] train_loss: 1.7107, valid_loss: 2.4221
[15/50] train_loss: 1.7013, valid_loss: 2.4056
[16/50] train_loss: 1.6830, valid_loss: 2.3900
[17/50] train_loss: 1.6828, valid_loss: 2.4200
[18/50] train_loss: 1.6572, valid_loss: 2.4330
[19/50] train_loss: 1.6530, valid_loss: 2.3797
[20/50] train_loss: 1.6415, valid_loss: 2.4353
[21/50] train_loss: 1.6510, valid_loss: 2.3817
[22/50] train_loss: 1.6354, valid_loss: 2.5030
[23/50] train_loss: 1.6300, valid_loss: 2.4365
[24/50] train_loss: 1.6253, valid_loss: 2.4061
[25/50] train_loss: 1.6184, valid_loss: 2.4195
[26/50] train_loss: 1.2683, valid_loss: 2.1627
[27/50] train_loss: 1.1397, valid_loss: 2.1417
[28/50] train_loss: 1.0859, valid_loss: 2.1415
[29/50] train_loss: 1.0483, valid_loss: 2.1268
[30/50] train_loss: 1.0165, valid_loss: 2.1201
[31/50] train_loss: 0.9905, valid_loss: 2.1506
[32/50] train_loss: 0.9642, valid_loss: 2.1266
[33/50] train_loss: 0.9431, valid_loss: 2.1473
[34/50] train_loss: 0.9238, valid_loss: 2.1309
[35/50] train_loss: 0.9062, valid_loss: 2.1685
[36/50] train_loss: 0.8891, valid_loss: 2.1484
[37/50] train_loss: 0.8728, valid_loss: 2.1533
[38/50] train_loss: 0.7399, valid_loss: 2.1394
[39/50] train_loss: 0.7096, valid_loss: 2.1316
[40/50] train_loss: 0.6956, valid_loss: 2.1490
[41/50] train_loss: 0.6867, valid_loss: 2.1400
[42/50] train_loss: 0.6791, valid_loss: 2.1384
[43/50] train_loss: 0.6721, valid_loss: 2.1678
[44/50] train_loss: 0.6664, valid_loss: 2.1506
[45/50] train_loss: 0.6597, valid_loss: 2.1707
[46/50] train_loss: 0.6518, valid_loss: 2.1507
[47/50] train_loss: 0.6467, valid_loss: 2.1509
[48/50] train_loss: 0.6404, valid_loss: 2.1605
[49/50] train_loss: 0.6340, valid_loss: 2.1537
[50/50] train_loss: 0.6271, valid_loss: 2.1632
