{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function, NestedIOFunction, Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn._functions.thnn import rnnFusedPointwise as fusedBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field, Iterator, BucketIterator, TabularDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torchnlp.metrics import get_moses_multi_bleu\n",
    "from NMTutils import get_parser, build_data, get_model_config, evaluation\n",
    "\n",
    "from decoder import Decoder\n",
    "from encoder import Encoder\n",
    "from attention import Attention\n",
    "# others\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE_CUDA = torch.cuda.is_available()\n",
    "# DEVICE = torch.cuda.current_device()\n",
    "USE_CUDA = False\n",
    "DEVICE = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = 'eng'\n",
    "lang2 = 'fra'\n",
    "modelcode_small = ['111301', '111311', '111301', '111311', '111311', \n",
    "                  '111311', '111311', '122421', '122521', '122621',\n",
    "                  '122622', '222421', '222521', '222421']\n",
    "modelcode_filtered = ['322521', '322421']\n",
    "\n",
    "model_idx = 1\n",
    "config, test_data, test_loader, SOURCE, TARGET = get_model_config(modelcode_small[model_idx-1], lang1, lang2,\n",
    "                                                                 device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    inputs, lengths = batch.so\n",
    "    targets = batch.ta\n",
    "    break\n",
    "lengths = lengths.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(len(SOURCE.vocab), 10)\n",
    "gru = nn.GRU(10, 7, 3, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 9, 10])\n",
      "torch.Size([6, 64, 7])\n",
      "torch.Size([64, 9, 14])\n",
      "torch.Size([64, 1, 14])\n"
     ]
    }
   ],
   "source": [
    "embeded = embed(inputs)\n",
    "print(embeded.size())\n",
    "packed = pack_padded_sequence(embeded, lengths, batch_first=True)\n",
    "outputs, hidden = gru(packed)\n",
    "print(hidden.size())\n",
    "outputs, output_lengths = pad_packed_sequence(outputs, batch_first=True)\n",
    "print(outputs.size())\n",
    "hidden = torch.cat([h for h in hidden[-2:]], 1).unsqueeze(1)\n",
    "print(hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 9, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "hidden_size = 6\n",
    "seq_len = 7\n",
    "bias = True\n",
    "B = 10\n",
    "x = torch.randn((seq_len, B, input_size))\n",
    "h = torch.randn((B, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ih = nn.Linear(input_size, 3*hidden_size, bias=bias)\n",
    "w_hh = nn.Linear(hidden_size, 3*hidden_size, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 18]) torch.Size([10, 18])\n",
      "torch.Size([10, 6]) torch.Size([10, 6])\n"
     ]
    }
   ],
   "source": [
    "gi = w_ih(x[0])\n",
    "gh = w_hh(h)\n",
    "print(gi.size(), gh.size())\n",
    "i_r, i_i, i_n = gi.chunk(3, 1)\n",
    "h_r, h_i, h_n = gh.chunk(3, 1)\n",
    "\n",
    "a_r = i_r + h_r\n",
    "a_i = i_i + h_i\n",
    "print(a_r.size(), a_i.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_r = nn.LayerNorm(hidden_size, elementwise_affine=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5299,  0.6143,  0.7059, -1.2720,  0.9255,  0.5562],\n",
       "        [-1.7578,  0.4601,  0.1428, -0.5778,  0.2171,  1.5156],\n",
       "        [ 0.6809,  0.7926,  0.9557, -1.9366, -0.4945,  0.0020],\n",
       "        [ 0.3680,  1.0435,  1.1615, -0.0356, -0.9473, -1.5900],\n",
       "        [ 0.7292, -0.2817, -1.1384, -1.3739,  1.2232,  0.8417],\n",
       "        [-0.6347,  0.0036, -1.4513, -0.3078,  1.7144,  0.6759],\n",
       "        [-1.5679,  1.6891,  0.6106, -0.5432, -0.1318, -0.0568],\n",
       "        [-1.8565,  1.3214,  0.7711, -0.3297,  0.2696, -0.1759],\n",
       "        [-1.6745, -0.0496,  0.5500, -0.8805,  0.9775,  1.0771],\n",
       "        [ 1.2672, -1.3910,  0.8129,  0.0841, -1.2502,  0.4769]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_r(a_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "grucell = nn.GRUCell(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "layernormgru = LayerNormGRU(input_size, hidden_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "hy = layernormgru.gru_cell(x[0], h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hy.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grucell(x[0], h).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=False, \n",
    "                 bidirectional=False, bias=True, use_cuda=False):\n",
    "        super(LayerNormGRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bidrectional = bidirectional\n",
    "        self.num_directions = 2 if self.bidrectional else 1\n",
    "        self.bias = bias\n",
    "        self.use_cuda = use_cuda\n",
    "        self.gate_num = 3\n",
    "        \n",
    "        self.weight_ih = nn.Linear(input_size, self.gate_num*hidden_size, bias=bias)\n",
    "        self.weight_hh = nn.Linear(hidden_size, self.gate_num*hidden_size, bias=bias)\n",
    "        if self.use_cuda:\n",
    "            self.weight_ih = self.weight_ih.cuda()\n",
    "            self.weight_hh = self.weight_hh.cuda()\n",
    "            \n",
    "        self.lm_r = nn.LayerNorm(hidden_size)\n",
    "        self.lm_i = nn.LayerNorm(hidden_size)\n",
    "        self.lm_n = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def gru_cell(self, inpt, hidden):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        * inpt: B, input_size\n",
    "        * hidden: B, hidden_size\n",
    "        output:\n",
    "        * \n",
    "        \"\"\"\n",
    "        gi = self.weight_ih(inpt)\n",
    "        gh = self.weight_hh(hidden)\n",
    "        i_r, i_i, i_n = gi.chunk(3, 1)\n",
    "        h_r, h_i, h_n = gh.chunk(3, 1)\n",
    "        \n",
    "        a_r = self.lm_r(i_r + h_r)\n",
    "        a_i = self.lm_i(i_i + h_i)\n",
    "        \n",
    "        resetgate = F.sigmoid(a_r)\n",
    "        inputgate = F.sigmoid(a_i)\n",
    "        \n",
    "        a_n = self.lm_n(i_n + resetgate * h_n)\n",
    "        newgate = F.tanh(a_n)\n",
    "        hy = newgate + inputgate * (hidden - newgate)\n",
    "        return hy\n",
    "    \n",
    "    def forward(self, inpt, hx=None):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        * inpt: seq_len, B, input_size\n",
    "        * hidden: B, hidden_size\n",
    "        output:\n",
    "        * output: seq_len, B, hidden_size * num_directions\n",
    "        * hidden: num_layers * num_directions, B, hidden_size\n",
    "        \"\"\"\n",
    "        if hx is None:\n",
    "            hx = init_hidden(inpt.size(1))\n",
    "        seq_len = inpt.size(0)\n",
    "        output = []\n",
    "        for t in range(seq_len):\n",
    "            \n",
    "\n",
    "        return out, hx\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h_0 = torch.zeros(batch_size, self.hidden_size)\n",
    "        if self.use_cuda:\n",
    "            h_0 = h_0.cuda()\n",
    "        return h_0\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, V_e, m_e, n_e, num_layers=1, bidrec=False, use_dropout=False, dropout_rate=0.5, \n",
    "                 layernorm=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"\"\"\n",
    "        vocab_size: V_e\n",
    "        embed_size: m_e\n",
    "        hidden_size: n_e\n",
    "        \"\"\"\n",
    "        self.V_e = V_e\n",
    "        self.m_e = m_e\n",
    "        self.n_e = n_e\n",
    "        self.num_layers = num_layers\n",
    "        self.bidrec = bidrec\n",
    "        self.n_direct = 2 if bidrec else 1\n",
    "        self.use_dropout = use_dropout\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        if self.use_dropout:\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        if self.layernorm:\n",
    "            self.lm = nn.LayerNorm()\n",
    "\n",
    "        self.embed = nn.Embedding(V_e, m_e) \n",
    "        self.gru = nn.GRU(m_e, n_e, num_layers, batch_first=True, bidirectional=bidrec)\n",
    "        \n",
    "    def forward(self, inputs, lengths):\n",
    "        \"\"\"\n",
    "        input: \n",
    "        - inputs: B, T_x\n",
    "        - lengths: actual max length of batches\n",
    "        output:\n",
    "        - outputs: B, T_x, n_e\n",
    "        \"\"\"\n",
    "        # embeded: (B, T_x, n_e)\n",
    "        embeded = self.embed(inputs)\n",
    "        if self.use_dropout:\n",
    "            embeded = self.dropout(embeded)\n",
    "            \n",
    "        # packed: (B*T_x, n_e)\n",
    "        packed = pack_padded_sequence(embeded, lengths, batch_first=True) \n",
    "        # packed outputs: (B*T_x, 2*n_e)\n",
    "        # hidden: (num of layers*n_direct, B, 2*n_e)\n",
    "        outputs, hidden = self.gru(packed)\n",
    "        # unpacked outputs: (B, T_x, 2*n_e)\n",
    "        outputs, output_lengths = pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        # hidden bidirection: (num of layers*n_direct(0,1,2...last one), B, n_e)\n",
    "        # choosen last hidden: (B, 1, 2*n_e)\n",
    "        hidden = torch.cat([h for h in hidden[-self.n_direct:]], 1).unsqueeze(1)\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(config, SOURCE, TARGET):\n",
    "    enc = Encoder(len(SOURCE.vocab), config.EMBED, config.HIDDEN, config.NUM_HIDDEN, bidrec=True)\n",
    "    dec = Decoder(len(TARGET.vocab), config.EMBED, 2*config.HIDDEN, hidden_size2=config.HIDDEN2, \\\n",
    "                  sos_idx=SOURCE.vocab.stoi['<s>'], method=config.METHOD, USE_CUDA=USE_CUDA)\n",
    "    if USE_CUDA:\n",
    "        enc = enc.cuda()\n",
    "        dec = dec.cuda()\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=TARGET.vocab.stoi['<pad>'])\n",
    "    return enc, dec, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
