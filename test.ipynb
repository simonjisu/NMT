{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function, NestedIOFunction, Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn._functions.thnn import rnnFusedPointwise as fusedBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field, Iterator, BucketIterator, TabularDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torchnlp.metrics import get_moses_multi_bleu\n",
    "from NMTutils import get_parser, build_data, get_model_config, evaluation\n",
    "\n",
    "from decoder import Decoder\n",
    "from encoder import Encoder\n",
    "from attention import Attention\n",
    "# others\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE_CUDA = torch.cuda.is_available()\n",
    "# DEVICE = torch.cuda.current_device()\n",
    "USE_CUDA = False\n",
    "DEVICE = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = 'eng'\n",
    "lang2 = 'fra'\n",
    "modelcode_small = ['111301', '111311', '111301', '111311', '111311', \n",
    "                  '111311', '111311', '122421', '122521', '122621',\n",
    "                  '122622', '222421', '222521', '222421']\n",
    "modelcode_filtered = ['322521', '322421']\n",
    "\n",
    "model_idx = 1\n",
    "config, test_data, test_loader, SOURCE, TARGET = get_model_config(modelcode_small[model_idx-1], lang1, lang2,\n",
    "                                                                 device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    inputs, lengths = batch.so\n",
    "    targets = batch.ta\n",
    "    break\n",
    "lengths = lengths.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/IBM/pytorch-seq2seq/blob/master/seq2seq/models/TopKDecoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_size = 10\n",
    "hid_size = 12\n",
    "embed = nn.Embedding(len(SOURCE.vocab), emd_size)\n",
    "gru = nn.GRU(emd_size, hid_size, 3, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dec = nn.Embedding(len(TARGET.vocab), emd_size)\n",
    "gru_dec = nn.GRU(emd_size+2*hid_size, 2*hid_size, 1, batch_first=True, bidirectional=False)\n",
    "linear = nn.Linear(2*2*hid_size, len(TARGET.vocab))\n",
    "attention = Attention(2*hid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos = torch.LongTensor([2]*inputs.size(0)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 9, 10])\n",
      "torch.Size([6, 64, 12])\n",
      "torch.Size([64, 9, 24])\n",
      "torch.Size([64, 1, 24])\n"
     ]
    }
   ],
   "source": [
    "embeded = embed(inputs)\n",
    "print(embeded.size())\n",
    "packed = pack_padded_sequence(embeded, lengths, batch_first=True)\n",
    "outputs, hidden = gru(packed)\n",
    "print(hidden.size())\n",
    "outputs, output_lengths = pad_packed_sequence(outputs, batch_first=True)\n",
    "print(outputs.size())\n",
    "hidden = torch.cat([h for h in hidden[-2:]], 1).unsqueeze(1)\n",
    "print(hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 10])\n",
      "torch.Size([64, 1, 34])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([64, 1, 48])\n"
     ]
    }
   ],
   "source": [
    "embeded_dec = embed_dec(sos)\n",
    "print(embeded_dec.size())\n",
    "context, weights = attention(hidden, outputs, output_lengths.tolist(), return_weight=True)\n",
    "gru_input = torch.cat([embeded_dec, context], 2)\n",
    "print(gru_input.size())\n",
    "_, hidden = gru_dec(gru_input, hidden.transpose(0, 1))\n",
    "print(hidden.size())\n",
    "hidden = hidden.transpose(0, 1)\n",
    "concated = torch.cat([hidden, context], 2)\n",
    "print(concated.size())\n",
    "score = linear(concated.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_idxes = defaultdict(dict)\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores, sorted_idxes = score.sort(dim=1, descending=True)\n",
    "ktop_idxes = sorted_idxes[:, :5]\n",
    "ktop_scores = sorted_scores[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.append(ktop_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_idxes[1]['score'] = ktop_scores\n",
    "beam_idxes[1]['idxes'] = ktop_idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, 10, (4, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  6.,  6.,  3.],\n",
       "        [ 1.,  3.,  1.,  0.,  2.],\n",
       "        [ 8.,  1.,  8.,  6.,  6.],\n",
       "        [ 4.,  0.,  6.,  8.,  9.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6.0359e-01,  5.8120e-01,  5.5645e-01,  ..., -6.0914e-01,\n",
       "          -6.5717e-01, -6.7255e-01],\n",
       "         [ 5.9112e-01,  5.7375e-01,  5.5910e-01,  ..., -6.0132e-01,\n",
       "          -6.3573e-01, -6.4145e-01],\n",
       "         [ 6.0446e-01,  5.8701e-01,  5.8411e-01,  ..., -6.1869e-01,\n",
       "          -6.6643e-01, -6.8972e-01],\n",
       "         ...,\n",
       "         [ 6.0359e-01,  5.8120e-01,  5.5645e-01,  ..., -6.0914e-01,\n",
       "          -6.5717e-01, -6.7255e-01],\n",
       "         [ 6.1332e-01,  5.4864e-01,  5.3648e-01,  ..., -5.7529e-01,\n",
       "          -6.4074e-01, -6.4379e-01],\n",
       "         [ 6.0347e-01,  6.0111e-01,  5.9818e-01,  ..., -6.1534e-01,\n",
       "          -6.7679e-01, -7.0563e-01]]),\n",
       " tensor([[ 9308,  7745,  5762,  ...,  4880,  1378,  1262],\n",
       "         [ 9308,  7745,  3605,  ...,  4880,  1378,  1262],\n",
       "         [ 9308,  7745,  2550,  ...,  4880,  1378,  1262],\n",
       "         ...,\n",
       "         [ 9308,  7745,  5762,  ...,  4880,  1378,  1262],\n",
       "         [ 9308,  7745,  4197,  ...,  4880,  1262,  1378],\n",
       "         [ 9308,  4197,  7745,  ...,  4880,  1378,  1262]]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.sort(dim=1, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "hidden_size = 6\n",
    "seq_len = 7\n",
    "bias = True\n",
    "B = 10\n",
    "num_layer = 2\n",
    "x = torch.randn((seq_len, B, input_size))\n",
    "h = torch.randn((num_layer*2, B, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ih = nn.Linear(input_size, 3*hidden_size, bias=bias)\n",
    "w_hh = nn.Linear(hidden_size, 3*hidden_size, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 18]) torch.Size([10, 18])\n",
      "torch.Size([10, 6]) torch.Size([10, 6])\n"
     ]
    }
   ],
   "source": [
    "gi = w_ih(x[0])\n",
    "gh = w_hh(h[0])\n",
    "print(gi.size(), gh.size())\n",
    "i_r, i_i, i_n = gi.chunk(3, 1)\n",
    "h_r, h_i, h_n = gh.chunk(3, 1)\n",
    "\n",
    "a_r = i_r + h_r\n",
    "a_i = i_i + h_i\n",
    "print(a_r.size(), a_i.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_r = nn.LayerNorm(hidden_size, elementwise_affine=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8483,  0.5183,  1.2515, -0.6050, -0.2809, -1.7322],\n",
       "        [ 1.1328, -0.1554, -0.1978, -0.2964, -1.7330,  1.2498],\n",
       "        [ 1.6602, -0.5964,  0.5125,  0.5637, -0.9766, -1.1634],\n",
       "        [-0.0735,  0.1378,  1.0084,  0.4167, -2.1003,  0.6110],\n",
       "        [-0.2187, -0.7102, -0.1035,  0.5517, -1.3435,  1.8242],\n",
       "        [ 0.5345,  1.8563, -0.5719, -0.5729,  0.0238, -1.2698],\n",
       "        [-0.1510, -0.0145, -0.5039,  1.1937,  1.1801, -1.7045],\n",
       "        [ 0.0017, -1.8937,  1.2953,  0.2352, -0.3739,  0.7355],\n",
       "        [ 1.0443,  0.2331,  1.0121,  0.2053, -0.6657, -1.8290],\n",
       "        [ 0.8642,  0.8665,  0.7942, -1.1174, -1.6072,  0.1996]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_r(a_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/pytorch/issues/4930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "import numpy as np\n",
    "\n",
    "class LayerNormGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=False, \n",
    "                 bidirectional=False, bias=True, use_cuda=False):\n",
    "        super(LayerNormGRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bidrectional = bidirectional\n",
    "        self.num_directions = 2 if self.bidrectional else 1\n",
    "        self.bias = bias\n",
    "        self.use_cuda = use_cuda\n",
    "        self.gate_num = 3\n",
    "        \n",
    "        self.weight_ih = nn.Linear(input_size, self.gate_num*hidden_size, bias=bias)\n",
    "        self.weight_hh = nn.Linear(hidden_size, self.gate_num*hidden_size, bias=bias)\n",
    "        if self.use_cuda:\n",
    "            self.weight_ih = self.weight_ih.cuda()\n",
    "            self.weight_hh = self.weight_hh.cuda()\n",
    "            \n",
    "        self.lm_r = nn.LayerNorm(hidden_size)\n",
    "        self.lm_i = nn.LayerNorm(hidden_size)\n",
    "        self.lm_n = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def gru_cell(self, inpt, hidden):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        * inpt: B, input_size\n",
    "        * hidden: B, hidden_size\n",
    "        output:\n",
    "        * \n",
    "        \"\"\"\n",
    "        gi = self.weight_ih(inpt)\n",
    "        gh = self.weight_hh(hidden)\n",
    "        i_r, i_i, i_n = gi.chunk(3, 1)\n",
    "        h_r, h_i, h_n = gh.chunk(3, 1)\n",
    "        \n",
    "        a_r = self.lm_r(i_r + h_r)\n",
    "        a_i = self.lm_i(i_i + h_i)\n",
    "        \n",
    "        resetgate = F.sigmoid(a_r)\n",
    "        inputgate = F.sigmoid(a_i)\n",
    "        \n",
    "        a_n = self.lm_n(i_n + resetgate * h_n)\n",
    "        newgate = F.tanh(a_n)\n",
    "        hy = newgate + inputgate * (hidden - newgate)\n",
    "        return hy\n",
    "    \n",
    "    def forward(self, inpt, hidden=None):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        * inpt: seq_len, B, input_size\n",
    "        * hidden: num_layers * num_directions, B, hidden_size\n",
    "        output:\n",
    "        * output: seq_len, B, hidden_size * num_directions\n",
    "        * hidden: num_layers * num_directions, B, hidden_size\n",
    "        \"\"\"\n",
    "        is_packed = isinstance(inpt, PackedSequence)\n",
    "        if is_packed:\n",
    "            inpt, batch_sizes = inpt\n",
    "            max_batch_size = int(batch_sizes[0])\n",
    "        else:\n",
    "            batch_sizes = None\n",
    "            max_batch_size = inpt.size(0) if self.batch_first else inpt.size(1)\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(inpt, max_batch_size)\n",
    "            \n",
    "        seq_len = inpt.size(0)\n",
    "        output_forward = []\n",
    "        for t in range(seq_len):\n",
    "            hidden = self.gru_cell(inpt[t], hidden)\n",
    "            # bidirec needed\n",
    "            output_forward.append(hidden)\n",
    "        output_forward = torch.stack(output)\n",
    "        \n",
    "        output = output_forward\n",
    "        if self.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, inpt, max_batch_size):\n",
    "        hx = inpt.new_zeros(self.num_layers * self.num_directions,\n",
    "                             max_batch_size, self.hidden_size,\n",
    "                             requires_grad=False)\n",
    "        if self.use_cuda:\n",
    "            hx = hx.cuda()\n",
    "        return hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = pack_padded_sequence(x, [7]*9 + [6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt, batch_sizes = packed\n",
    "max_batch_size = int(batch_sizes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([69, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpt.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hx = inpt.new_zeros(num_layer * 2, max_batch_size, hidden_size, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 6])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hx.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grucell = nn.GRUCell(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "layernormgru = LayerNormGRU(input_size, hidden_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "hy = layernormgru.gru_cell(x[0], h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hy.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grucell(x[0], h).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (18) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-4700aa72da4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayernormgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-3e9cfd12e884>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inpt, hidden)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0moutput_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;31m# bidirec needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0moutput_forward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-3e9cfd12e884>\u001b[0m in \u001b[0;36mgru_cell\u001b[0;34m(self, inpt, hidden)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mh_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0ma_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_r\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_r\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0ma_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (18) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "out, hid = layernormgru(x.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt, batch_sizes = packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, V_e, m_e, n_e, num_layers=1, bidrec=False, use_dropout=False, dropout_rate=0.5, \n",
    "                 layernorm=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"\"\"\n",
    "        vocab_size: V_e\n",
    "        embed_size: m_e\n",
    "        hidden_size: n_e\n",
    "        \"\"\"\n",
    "        self.V_e = V_e\n",
    "        self.m_e = m_e\n",
    "        self.n_e = n_e\n",
    "        self.num_layers = num_layers\n",
    "        self.bidrec = bidrec\n",
    "        self.n_direct = 2 if bidrec else 1\n",
    "        self.use_dropout = use_dropout\n",
    "        self.layernorm = layernorm\n",
    "\n",
    "        if self.use_dropout:\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        if self.layernorm:\n",
    "            self.lm = nn.LayerNorm()\n",
    "\n",
    "        self.embed = nn.Embedding(V_e, m_e) \n",
    "        self.gru = nn.GRU(m_e, n_e, num_layers, batch_first=True, bidirectional=bidrec)\n",
    "        \n",
    "    def forward(self, inputs, lengths):\n",
    "        \"\"\"\n",
    "        input: \n",
    "        - inputs: B, T_x\n",
    "        - lengths: actual max length of batches\n",
    "        output:\n",
    "        - outputs: B, T_x, n_e\n",
    "        \"\"\"\n",
    "        # embeded: (B, T_x, n_e)\n",
    "        embeded = self.embed(inputs)\n",
    "        if self.use_dropout:\n",
    "            embeded = self.dropout(embeded)\n",
    "            \n",
    "        # packed: (B*T_x, n_e)\n",
    "        packed = pack_padded_sequence(embeded, lengths, batch_first=True) \n",
    "        # packed outputs: (B*T_x, 2*n_e)\n",
    "        # hidden: (num of layers*n_direct, B, 2*n_e)\n",
    "        outputs, hidden = self.gru(packed)\n",
    "        # unpacked outputs: (B, T_x, 2*n_e)\n",
    "        outputs, output_lengths = pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        # hidden bidirection: (num of layers*n_direct(0,1,2...last one), B, n_e)\n",
    "        # choosen last hidden: (B, 1, 2*n_e)\n",
    "        hidden = torch.cat([h for h in hidden[-self.n_direct:]], 1).unsqueeze(1)\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(config, SOURCE, TARGET):\n",
    "    enc = Encoder(len(SOURCE.vocab), config.EMBED, config.HIDDEN, config.NUM_HIDDEN, bidrec=True)\n",
    "    dec = Decoder(len(TARGET.vocab), config.EMBED, 2*config.HIDDEN, hidden_size2=config.HIDDEN2, \\\n",
    "                  sos_idx=SOURCE.vocab.stoi['<s>'], method=config.METHOD, USE_CUDA=USE_CUDA)\n",
    "    if USE_CUDA:\n",
    "        enc = enc.cuda()\n",
    "        dec = dec.cuda()\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=TARGET.vocab.stoi['<pad>'])\n",
    "    return enc, dec, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
